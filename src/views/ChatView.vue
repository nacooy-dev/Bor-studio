<template>
  <div class="h-screen flex flex-col bg-transparent overflow-hidden">
    <!-- æç®€èŠå¤©ç•Œé¢ - å®Œå…¨çº¯å‡€ï¼Œæ— ä»»ä½•è£…é¥° -->
    <div class="flex-1 flex flex-col max-w-7xl mx-auto w-full px-4 py-6 min-h-0">
      
      <!-- æç®€æ¬¢è¿ä¿¡æ¯ (ä»…åœ¨æ²¡æœ‰æ¶ˆæ¯æ—¶æ˜¾ç¤º) -->
      <div v-if="messages.length === 0" class="flex-1 flex items-center justify-center">
        <div class="text-center animate-fade-in max-w-lg">
          <!-- ç®€æ´çš„å›¾æ ‡ -->
          <div class="w-16 h-16 mx-auto mb-8 glass rounded-2xl flex items-center justify-center">
            <div class="w-8 h-8 bg-gradient-to-br from-primary-blue to-primary-indigo rounded-lg"></div>
          </div>
          
          <!-- ç®€æ´çš„æ ‡é¢˜ -->
          <h1 class="text-3xl font-light text-neutral-gray-800 dark:text-neutral-gray-200 mb-3">
            Bor
          </h1>
          <p class="text-neutral-gray-500 dark:text-neutral-gray-400 mb-8 text-lg font-light">
            é€šè¿‡å¯¹è¯æ§åˆ¶ä¸€åˆ‡
          </p>
          
          <!-- å»ºè®®æŒ‰é’® - æ›´ç®€æ´çš„æ ·å¼ -->
          <div class="flex flex-wrap gap-3 justify-center">
            <button
              v-for="suggestion in suggestions"
              :key="suggestion"
              @click="sendSuggestion(suggestion)"
              class="glass rounded-full px-4 py-2 text-sm font-medium text-neutral-gray-700 dark:text-neutral-gray-300 hover:bg-opacity-80 transition-all duration-200 hover:scale-105"
            >
              {{ suggestion }}
            </button>
            
            <!-- æµ‹è¯•é…ç½®æŒ‰é’® -->
            <button
              @click="$router.push('/config')"
              class="glass rounded-full px-4 py-2 text-sm font-medium text-blue-600 hover:bg-opacity-80 transition-all duration-200 hover:scale-105"
            >
              ğŸ”§ æ‰“å¼€é…ç½®
            </button>
          </div>
        </div>
      </div>
      
      <!-- æ¶ˆæ¯åˆ—è¡¨ - æç®€æ»šåŠ¨åŒºåŸŸ -->
      <div
        v-else
        ref="messagesContainer"
        class="flex-1 overflow-y-auto custom-scrollbar space-y-6 py-6 min-h-0"
      >
        <div
          v-for="message in messages"
          :key="message.id"
          class="animate-slide-up"
        >
          <ChatMessage 
            :message="message" 
            :is-streaming="message.id === streamingMessageId"
          />
        </div>
        
        <!-- ä¼˜é›…çš„è¾“å…¥æŒ‡ç¤ºå™¨ -->
        <div v-if="isLoading" class="flex justify-start">
          <div class="glass rounded-2xl px-4 py-3 max-w-xs">
            <div class="flex items-center space-x-2">
              <div class="flex space-x-1">
                <div class="w-2 h-2 bg-neutral-gray-400 rounded-full animate-pulse-soft"></div>
                <div class="w-2 h-2 bg-neutral-gray-400 rounded-full animate-pulse-soft" style="animation-delay: 0.2s"></div>
                <div class="w-2 h-2 bg-neutral-gray-400 rounded-full animate-pulse-soft" style="animation-delay: 0.4s"></div>
              </div>
              <span class="text-xs text-neutral-gray-500">æ­£åœ¨æ€è€ƒ...</span>
            </div>
          </div>
        </div>
      </div>
      
      <!-- è¾“å…¥åŒºåŸŸ - å›ºå®šåœ¨åº•éƒ¨ -->
      <div class="flex-shrink-0 mt-4">
        <ChatInput
          v-model="inputText"
          :is-loading="isLoading"
          :placeholder="getContextualPlaceholder()"
          @send="handleSend"
          @file-drop="handleFileDrop"
          @stop="handleStop"
        />
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, nextTick, onMounted } from 'vue'
import { useRouter } from 'vue-router'
import { useChatStore } from '@/stores/chat'
import { llmManager } from '@/lib/llm-manager'
import { DialogueRouter } from '@/services/dialogue/DialogueRouter'
import { MessageFactory } from '@/utils/messageFactory'
import ChatMessage from '@/components/ChatMessage.vue'
import ChatInput from '@/components/ChatInput.vue'
import type { Message } from '@/types'

const $router = useRouter()
const chatStore = useChatStore()
// ä½¿ç”¨å…¨å±€ LLM ç®¡ç†å™¨å®ä¾‹
const dialogueRouter = new DialogueRouter()
const messagesContainer = ref<HTMLElement>()
const inputText = ref('')
const isLoading = ref(false)
const streamingMessageId = ref<string | null>(null)
const abortController = ref<AbortController | null>(null)
const systemStatus = ref({
  ollama: false,
  currentModel: '',
  availableModels: [] as string[]
})

// ä» store è·å–æ¶ˆæ¯
const messages = ref<Message[]>([])

// å»ºè®®å¿«æ·æŒ‡ä»¤ - æ ¹æ®ç³»ç»ŸçŠ¶æ€åŠ¨æ€è°ƒæ•´
const getSuggestions = () => {
  if (!systemStatus.value.ollama) {
    return ['æ£€æŸ¥ç³»ç»ŸçŠ¶æ€', 'é…ç½® LLM', 'å¦‚ä½•å®‰è£… Ollamaï¼Ÿ']
  } else if (systemStatus.value.availableModels.length === 0) {
    return ['åˆ·æ–°æ¨¡å‹åˆ—è¡¨', 'å¦‚ä½•æ‹‰å–æ¨¡å‹ï¼Ÿ', 'æ£€æŸ¥ç³»ç»ŸçŠ¶æ€']
  } else {
    return ['ä½ å¥½', 'å¸®æˆ‘å†™ä»£ç ', 'æ£€æŸ¥ç³»ç»ŸçŠ¶æ€', 'é…ç½® LLM']
  }
}

const suggestions = ref(getSuggestions())

// è·å–ä¸Šä¸‹æ–‡ç›¸å…³çš„å ä½ç¬¦
const getContextualPlaceholder = () => {
  if (messages.value.length === 0) {
    const placeholders = [
      'å¼€å§‹å¯¹è¯ï¼Œæˆ–è¯´"é…ç½® LLM"æ¥è®¾ç½®æ¨¡å‹...',
      'è¯•è¯•è¯´"ä½ å¥½"æˆ–"é…ç½® LLM"...',
      'é€šè¿‡å¯¹è¯æ§åˆ¶ä¸€åˆ‡åŠŸèƒ½...',
      'è¯´"é…ç½® LLM"å¼€å§‹è®¾ç½®ï¼Œæˆ–ç›´æ¥èŠå¤©...'
    ]
    return placeholders[Math.floor(Math.random() * placeholders.length)]
  }
  
  // æ ¹æ®æœ€åå‡ æ¡æ¶ˆæ¯çš„å†…å®¹æä¾›æ™ºèƒ½æç¤º
  const lastMessage = messages.value[messages.value.length - 1]
  if (lastMessage?.role === 'assistant') {
    if (lastMessage.content.includes('é…ç½®')) {
      return 'ç»§ç»­é…ç½®ï¼Œæˆ–é—®å…¶ä»–é—®é¢˜...'
    }
    if (lastMessage.content.includes('å·¥ä½œæµ')) {
      return 'ç»§ç»­è®¨è®ºå·¥ä½œæµï¼Œæˆ–è¯´å…¶ä»–éœ€æ±‚...'
    }
    if (lastMessage.content.includes('æ–‡æ¡£') || lastMessage.content.includes('çŸ¥è¯†åº“')) {
      return 'ç»§ç»­è®¨è®ºæ–‡æ¡£ç®¡ç†ï¼Œæˆ–é—®å…¶ä»–é—®é¢˜...'
    }
  }
  
  return 'ç»§ç»­å¯¹è¯...'
}

// å‘é€å»ºè®®
const sendSuggestion = (suggestion: string) => {
  inputText.value = suggestion
  handleSend(suggestion)
}

// æµ‹è¯•é…ç½®è·³è½¬
const testConfigJump = () => {
  console.log('æµ‹è¯•é…ç½®è·³è½¬')
  $router.push('/config')
}

// å¤„ç†æ¶ˆæ¯å‘é€
const handleSend = async (content: string, files?: File[]) => {
  // ä½¿ç”¨ä¼ å…¥çš„contentï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨inputText.value
  const messageContent = content || inputText.value.trim()
  
  if (!messageContent || isLoading.value) return
  
  console.log('handleSend å¼€å§‹å¤„ç†:', { content: messageContent, isLoading: isLoading.value })
  
  // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
  const userMessage: Message = {
    id: Date.now().toString(),
    role: 'user',
    content: messageContent,
    timestamp: Date.now(),
  }
  
  messages.value.push(userMessage)
  inputText.value = ''
  isLoading.value = true
  
  // åˆ›å»ºä¸­æ­¢æ§åˆ¶å™¨
  abortController.value = new AbortController()
  
  // æ»šåŠ¨åˆ°åº•éƒ¨
  await nextTick()
  scrollToBottom()
  
  try {
    // è°ƒç”¨çœŸå®çš„ AI å“åº”å¤„ç†
    await handleAIResponse(messageContent)
  } catch (error) {
    console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error)
    
    // æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·ä¸»åŠ¨ä¸­æ­¢
    if (error.name === 'AbortError') {
      const abortMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: 'â¹ï¸ å¯¹è¯å·²ä¸­æ­¢',
        timestamp: Date.now(),
      }
      messages.value.push(abortMessage)
    } else {
      // æ·»åŠ é”™è¯¯æ¶ˆæ¯
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: 'æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚',
        timestamp: Date.now(),
      }
      messages.value.push(errorMessage)
    }
  } finally {
    isLoading.value = false
    abortController.value = null
    streamingMessageId.value = null
    await nextTick()
    scrollToBottom()
  }
}

// æ™ºèƒ½ AI å“åº”å¤„ç†
const handleAIResponse = async (userInput: string) => {
  try {
    // è·å–å¯¹è¯å†å²
    const conversationHistory = messages.value
      .slice(-10)
      .filter(msg => msg.role !== 'system')

    // ä½¿ç”¨æ™ºèƒ½å¯¹è¯è·¯ç”±
    const dialogueResponse = await dialogueRouter.routeDialogue(userInput, conversationHistory)
    
    console.log('å¯¹è¯è·¯ç”±ç»“æœ:', dialogueResponse)

    // å¦‚æœéœ€è¦LLMå¤„ç†
    if (dialogueResponse.metadata?.requiresLLM) {
      await handleLLMResponse(userInput, dialogueResponse)
      return
    }

    // å¦‚æœæœ‰ç›´æ¥å“åº”æ¶ˆæ¯
    if (dialogueResponse.message) {
      const assistantMessage = MessageFactory.createAssistantMessage(dialogueResponse.message)
      messages.value.push(assistantMessage)
    }

    // æ‰§è¡ŒåŠ¨ä½œ
    if (dialogueResponse.actions) {
      await executeDialogueActions(dialogueResponse.actions)
    }

    // æ˜¾ç¤ºåç»­é—®é¢˜
    if (dialogueResponse.followUpQuestions && dialogueResponse.followUpQuestions.length > 0) {
      // å¯ä»¥åœ¨è¿™é‡Œæ˜¾ç¤ºå¿«æ·å›å¤æŒ‰é’®
      console.log('åç»­é—®é¢˜:', dialogueResponse.followUpQuestions)
    }

  } catch (error) {
    console.error('æ™ºèƒ½å¯¹è¯å¤„ç†å¤±è´¥:', error)
    
    // å›é€€åˆ°ä¼ ç»ŸLLMå¤„ç†
    await handleLLMResponse(userInput)
  }
}

// å¤„ç†LLMå“åº”
const handleLLMResponse = async (userInput: string, dialogueResponse?: any) => {
  // æ£€æŸ¥ Ollama çŠ¶æ€
  if (!systemStatus.value.ollama) {
    const errorMessage = MessageFactory.createAssistantMessage(
      'âŒ Ollama æœåŠ¡æœªè¿æ¥\n\nè¯·ç¡®ä¿ï¼š\n1. Ollama å·²å®‰è£…å¹¶è¿è¡Œ\n2. æœåŠ¡åœ°å€ä¸º http://localhost:11434\n3. è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹å¯ç”¨\n\næ‚¨å¯ä»¥è¯´"æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"æ¥é‡æ–°æ£€æµ‹ã€‚'
    )
    messages.value.push(errorMessage)
    return
  }

  // æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨æ¨¡å‹
  if (systemStatus.value.availableModels.length === 0) {
    const errorMessage = MessageFactory.createAssistantMessage(
      'âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹\n\nè¯·å…ˆæ‹‰å–ä¸€ä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚ï¼š\n```bash\nollama pull llama2\n```\n\nç„¶åè¯´"åˆ·æ–°æ¨¡å‹åˆ—è¡¨"æ¥é‡æ–°æ£€æµ‹ã€‚'
    )
    messages.value.push(errorMessage)
    return
  }

  // æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦è®¾ç½®
  if (!systemStatus.value.currentModel) {
    console.log('å½“å‰æ¨¡å‹æœªè®¾ç½®ï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹')
    systemStatus.value.currentModel = systemStatus.value.availableModels[0]
    llmManager.setCurrentModel(systemStatus.value.currentModel)
  }

  console.log('å‡†å¤‡è°ƒç”¨ LLMï¼Œå½“å‰æ¨¡å‹:', systemStatus.value.currentModel)

  // åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯
  const assistantMessage = MessageFactory.createAssistantMessage('')
  messages.value.push(assistantMessage)
  streamingMessageId.value = assistantMessage.id

  try {
    // è·å–å¯¹è¯å†å²ï¼ˆæœ€è¿‘10æ¡æ¶ˆæ¯ï¼‰
    const conversationHistory = messages.value
      .slice(-10)
      .filter(msg => msg.role !== 'system')

    console.log('å¼€å§‹è°ƒç”¨ LLM:', {
      model: systemStatus.value.currentModel,
      messageCount: conversationHistory.length
    })

    // è°ƒç”¨ LLM
    const response = await llmManager.chat(conversationHistory, {
      onStream: (chunk: string) => {
        // æ‰¾åˆ°æ¶ˆæ¯åœ¨æ•°ç»„ä¸­çš„ç´¢å¼•å¹¶æ›´æ–°
        const messageIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
        if (messageIndex !== -1) {
          messages.value[messageIndex].content += chunk
        }
        // æ»šåŠ¨åˆ°åº•éƒ¨
        nextTick().then(() => scrollToBottom())
      },
      signal: abortController.value?.signal
    })

    console.log('LLM è°ƒç”¨å®Œæˆ')

    // å¦‚æœæ²¡æœ‰æµå¼å“åº”ï¼Œç›´æ¥è®¾ç½®å®Œæ•´å“åº”
    const messageIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
    if (messageIndex !== -1 && !messages.value[messageIndex].content && response) {
      messages.value[messageIndex].content = response
    }

  } catch (error) {
    console.error('LLM è°ƒç”¨å¤±è´¥:', error)
    const messageIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
    if (messageIndex !== -1) {
      messages.value[messageIndex].content = `âŒ å¯¹è¯å¤±è´¥ï¼š${error.message}\n\nè¯·æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€æˆ–å°è¯•åˆ‡æ¢æ¨¡å‹ã€‚`
    }
  } finally {
    streamingMessageId.value = null
  }
}

// æ‰§è¡Œå¯¹è¯åŠ¨ä½œ
const executeDialogueActions = async (actions: any[]) => {
  for (const action of actions) {
    console.log('æ‰§è¡ŒåŠ¨ä½œ:', action)
    
    switch (action.type) {
      case 'open_config':
        // é…ç½®çª—å£å·²ç»åœ¨å¯¹è¯å¤„ç†å™¨ä¸­æ‰“å¼€ï¼Œè¿™é‡Œåªéœ€è¦ç¡®è®¤
        console.log(`é…ç½®çª—å£å·²æ‰“å¼€: ${action.payload.configType}`)
        break
        
      case 'redirect':
        // å¤„ç†é¡µé¢è·³è½¬
        console.log('æ‰§è¡Œé¡µé¢è·³è½¬:', action.payload.url)
        if (action.payload.url) {
          try {
            // ä½¿ç”¨ Vue Router è¿›è¡Œè·³è½¬
            const targetPath = action.payload.url.replace('#', '')
            console.log('è·³è½¬åˆ°è·¯å¾„:', targetPath)
            await $router.push(targetPath)
            console.log('è·³è½¬æˆåŠŸ')
          } catch (error) {
            console.error('è·³è½¬å¤±è´¥:', error)
            // å›é€€åˆ°ç›´æ¥ä¿®æ”¹ hash
            window.location.hash = action.payload.url
          }
        }
        break
        
      case 'switch_model':
        if (action.payload.model) {
          systemStatus.value.currentModel = action.payload.model
          await llmManager.setModel(action.payload.model)
          
          const confirmMessage = MessageFactory.createAssistantMessage(
            `âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹ï¼š**${action.payload.model}**`
          )
          messages.value.push(confirmMessage)
        }
        break
        
      case 'execute_command':
        await executeSystemCommand(action.payload.command, action.payload.args)
        break
        
      case 'show_ui':
        console.log('æ˜¾ç¤ºUIç»„ä»¶:', action.payload.component)
        // è¿™é‡Œå¯ä»¥æ·»åŠ UIç»„ä»¶æ˜¾ç¤ºé€»è¾‘
        break
        
      default:
        console.log('æœªçŸ¥åŠ¨ä½œç±»å‹:', action.type)
    }
  }
}

// æ‰§è¡Œç³»ç»Ÿå‘½ä»¤
const executeSystemCommand = async (command: string, args: any[] = []) => {
  console.log('æ‰§è¡Œç³»ç»Ÿå‘½ä»¤:', command, args)
  
  switch (command) {
    case 'setTheme':
      const theme = args[0]
      if (window.electronAPI && window.electronAPI.setTheme) {
        try {
          await window.electronAPI.setTheme(theme)
          console.log(`ä¸»é¢˜å·²åˆ‡æ¢åˆ°: ${theme}`)
        } catch (error) {
          console.error('ä¸»é¢˜åˆ‡æ¢å¤±è´¥:', error)
        }
      } else {
        // åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­åˆ‡æ¢ä¸»é¢˜
        if (theme === 'dark') {
          document.documentElement.classList.add('dark')
          document.documentElement.classList.remove('light')
        } else {
          document.documentElement.classList.add('light')
          document.documentElement.classList.remove('dark')
        }
        document.documentElement.setAttribute('data-theme', theme)
        localStorage.setItem('bor-theme', theme)
        console.log(`æµè§ˆå™¨ä¸»é¢˜å·²åˆ‡æ¢åˆ°: ${theme}`)
        
        // ç«‹å³åé¦ˆç»™ç”¨æˆ·
        const themeMessage = MessageFactory.createAssistantMessage(
          `ğŸ¨ ä¸»é¢˜å·²ç«‹å³åˆ‡æ¢åˆ°${theme === 'dark' ? 'æ·±è‰²' : 'æµ…è‰²'}æ¨¡å¼ï¼`
        )
        messages.value.push(themeMessage)
      }
      break
      
    case 'refreshModels':
      await checkSystemStatus()
      const refreshMessage = MessageFactory.createAssistantMessage(
        `ğŸ”„ æ¨¡å‹åˆ—è¡¨å·²åˆ·æ–°ï¼Œå‘ç° ${systemStatus.value.availableModels.length} ä¸ªå¯ç”¨æ¨¡å‹`
      )
      messages.value.push(refreshMessage)
      break
      
    default:
      console.log('æœªçŸ¥ç³»ç»Ÿå‘½ä»¤:', command)
  }
}

// å¤„ç†ç³»ç»Ÿå‘½ä»¤
const handleSystemCommands = async (userInput: string): Promise<boolean> => {
  const input = userInput.toLowerCase()

  // é…ç½® LLM
  if (input.includes('é…ç½®') && input.includes('llm')) {
    const configMessage = MessageFactory.createConfigMessage('llm-settings', 'open')
    messages.value.push(configMessage)
    
    setTimeout(() => {
      if (window.electronAPI) {
        window.electronAPI.openConfigWindow('llm-settings')
      } else {
        // Web ç¯å¢ƒä¸‹çš„å¤„ç†
        alert('é…ç½®åŠŸèƒ½éœ€è¦åœ¨æ¡Œé¢åº”ç”¨ä¸­ä½¿ç”¨')
      }
    }, 1000)
    return true
  }

  // æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
  if (input.includes('æ£€æŸ¥') && (input.includes('çŠ¶æ€') || input.includes('ç³»ç»Ÿ'))) {
    await checkSystemStatus()
    const status = systemStatus.value
    const statusMessage = MessageFactory.createAssistantMessage(
      `ğŸ“Š ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š\n\n**Ollama æœåŠ¡ï¼š** ${status.ollama ? 'âœ… å·²è¿æ¥' : 'âŒ æœªè¿æ¥'}\n**å½“å‰æ¨¡å‹ï¼š** ${status.currentModel}\n**å¯ç”¨æ¨¡å‹ï¼š** ${status.availableModels.length} ä¸ª\n\n${status.availableModels.length > 0 ? `æ¨¡å‹åˆ—è¡¨ï¼š\n${status.availableModels.map(m => `- ${m}`).join('\n')}` : 'è¯·å…ˆæ‹‰å–æ¨¡å‹æ‰èƒ½å¼€å§‹å¯¹è¯ã€‚'}`
    )
    messages.value.push(statusMessage)
    return true
  }

  // åˆ·æ–°æ¨¡å‹åˆ—è¡¨
  if (input.includes('åˆ·æ–°') && input.includes('æ¨¡å‹')) {
    await checkSystemStatus()
    const refreshMessage = MessageFactory.createAssistantMessage(
      `ğŸ”„ æ¨¡å‹åˆ—è¡¨å·²åˆ·æ–°\n\nå‘ç° ${systemStatus.value.availableModels.length} ä¸ªå¯ç”¨æ¨¡å‹ï¼š\n${systemStatus.value.availableModels.map(m => `- ${m}`).join('\n') || 'æš‚æ— å¯ç”¨æ¨¡å‹'}`
    )
    messages.value.push(refreshMessage)
    return true
  }

  // åˆ‡æ¢æ¨¡å‹
  if (input.includes('åˆ‡æ¢æ¨¡å‹') || input.includes('ä½¿ç”¨æ¨¡å‹')) {
    const availableModels = systemStatus.value.availableModels
    if (availableModels.length === 0) {
      const errorMessage = MessageFactory.createAssistantMessage('âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆæ‹‰å–æ¨¡å‹ã€‚')
      messages.value.push(errorMessage)
      return true
    }

    // æ”¹è¿›çš„æ¨¡å‹é€‰æ‹©é€»è¾‘
    let modelName = null
    
    // å°è¯•ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ¨¡å‹åç§°
    const inputWords = userInput.toLowerCase().split(/\s+/)
    
    // æŸ¥æ‰¾å®Œå…¨åŒ¹é…çš„æ¨¡å‹
    for (const model of availableModels) {
      const modelLower = model.toLowerCase()
      if (userInput.toLowerCase().includes(modelLower)) {
        modelName = model
        break
      }
    }
    
    // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œå…¨åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
    if (!modelName) {
      for (const word of inputWords) {
        for (const model of availableModels) {
          if (model.toLowerCase().includes(word) && word.length > 2) {
            modelName = model
            break
          }
        }
        if (modelName) break
      }
    }
    
    // å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œæ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨
    if (!modelName) {
      const modelListMessage = MessageFactory.createAssistantMessage(
        `â“ è¯·æŒ‡å®šè¦åˆ‡æ¢çš„æ¨¡å‹åç§°\n\n**å¯ç”¨æ¨¡å‹ï¼š**\n${availableModels.map(m => `- ${m}`).join('\n')}\n\nä¾‹å¦‚ï¼šè¯´"åˆ‡æ¢æ¨¡å‹åˆ° ${availableModels[0]}"`
      )
      messages.value.push(modelListMessage)
      return true
    }
    
    systemStatus.value.currentModel = modelName
    llmManager.setCurrentModel(modelName)

    const switchMessage = MessageFactory.createAssistantMessage(
      `âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹ï¼š**${modelName}**\n\nç°åœ¨å¯ä»¥å¼€å§‹å¯¹è¯äº†ï¼`
    )
    messages.value.push(switchMessage)
    return true
  }

  // å¦‚ä½•å®‰è£… Ollama
  if (input.includes('å¦‚ä½•') && input.includes('å®‰è£…') && input.includes('ollama')) {
    const installMessage = MessageFactory.createAssistantMessage(
      `ğŸ“¦ å¦‚ä½•å®‰è£… Ollama\n\n**æ–¹æ³•ä¸€ï¼šå®˜ç½‘ä¸‹è½½**\n1. è®¿é—® https://ollama.ai\n2. ä¸‹è½½é€‚åˆæ‚¨ç³»ç»Ÿçš„å®‰è£…åŒ…\n3. æŒ‰ç…§å®‰è£…å‘å¯¼å®Œæˆå®‰è£…\n\n**æ–¹æ³•äºŒï¼šå‘½ä»¤è¡Œå®‰è£…**\n\`\`\`bash\n# macOS/Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Windows (PowerShell)\niwr -useb https://ollama.ai/install.ps1 | iex\n\`\`\`\n\n**å®‰è£…å®Œæˆåï¼š**\n1. æ‹‰å–ä¸€ä¸ªæ¨¡å‹ï¼š\`ollama pull llama2\`\n2. è¯´"æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"æ¥éªŒè¯å®‰è£…`
    )
    messages.value.push(installMessage)
    return true
  }

  // å¦‚ä½•æ‹‰å–æ¨¡å‹
  if (input.includes('å¦‚ä½•') && input.includes('æ‹‰å–') && input.includes('æ¨¡å‹')) {
    const pullMessage = MessageFactory.createAssistantMessage(
      `ğŸ”½ å¦‚ä½•æ‹‰å–æ¨¡å‹\n\n**æ¨èæ¨¡å‹ï¼š**\n\`\`\`bash\n# è½»é‡çº§æ¨¡å‹ï¼ˆæ¨èæ–°æ‰‹ï¼‰\nollama pull llama2:7b\nollama pull qwen:7b\n\n# ä¸­ç­‰æ¨¡å‹ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰\nollama pull llama2:13b\nollama pull mistral:7b\n\n# ä»£ç ä¸“ç”¨æ¨¡å‹\nollama pull codellama:7b\nollama pull deepseek-coder:6.7b\n\`\`\`\n\n**æ‹‰å–å®Œæˆåï¼š**\nè¯´"åˆ·æ–°æ¨¡å‹åˆ—è¡¨"æ¥é‡æ–°æ£€æµ‹å¯ç”¨æ¨¡å‹ã€‚`
    )
    messages.value.push(pullMessage)
    return true
  }

  return false
}

// å¤„ç†ä¸­æ­¢
const handleStop = () => {
  console.log('ç”¨æˆ·è¯·æ±‚ä¸­æ­¢å¯¹è¯')
  
  if (abortController.value) {
    abortController.value.abort()
    console.log('å·²å‘é€ä¸­æ­¢ä¿¡å·')
  }
  
  // ç«‹å³åœæ­¢åŠ è½½çŠ¶æ€
  isLoading.value = false
  streamingMessageId.value = null
  
  // æ·»åŠ ä¸­æ­¢æ¶ˆæ¯
  const stopMessage: Message = {
    id: Date.now().toString(),
    role: 'assistant',
    content: 'â¹ï¸ å¯¹è¯å·²ä¸­æ­¢',
    timestamp: Date.now(),
  }
  messages.value.push(stopMessage)
  
  nextTick().then(() => scrollToBottom())
}

// å¤„ç†æ–‡ä»¶æ‹–æ‹½
const handleFileDrop = (files: File[]) => {
  console.log('æ–‡ä»¶æ‹–æ‹½:', files)
  // è¿™é‡Œå°†å®ç°æ–‡ä»¶å¤„ç†é€»è¾‘
}

// æ»šåŠ¨åˆ°åº•éƒ¨
const scrollToBottom = () => {
  if (messagesContainer.value) {
    // ä½¿ç”¨ requestAnimationFrame ç¡®ä¿ DOM æ›´æ–°å®Œæˆåå†æ»šåŠ¨
    requestAnimationFrame(() => {
      if (messagesContainer.value) {
        messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
      }
    })
  }
}

// æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
const checkSystemStatus = async () => {
  try {
    await llmManager.initialize()
    const status = llmManager.getStatus()
    
    console.log('ğŸ” LLMManager è¿”å›çš„çŠ¶æ€:', status)
    
    systemStatus.value = {
      ollama: status.availableProviders.find(p => p.type === 'ollama')?.isAvailable || false,
      currentModel: status.currentModel,
      availableModels: status.availableModels.map(m => m.name)
    }
    
    console.log('ğŸ” è®¾ç½®åçš„ systemStatus:', systemStatus.value)
  } catch (error) {
    console.error('æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å¤±è´¥:', error)
    systemStatus.value = {
      ollama: false,
      currentModel: '',
      availableModels: []
    }
  }
}

onMounted(async () => {
  // åˆå§‹åŒ–èŠå¤©å­˜å‚¨
  chatStore.initialize()
  
  // æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
  await checkSystemStatus()
  
  // æ ¹æ®ç³»ç»ŸçŠ¶æ€æ˜¾ç¤ºä¸åŒçš„æ¬¢è¿ä¿¡æ¯
  if (messages.value.length === 0) {
    let welcomeMessage: Message
    
    if (!systemStatus.value.ollama) {
      // Ollama æœªè¿æ¥
      welcomeMessage = MessageFactory.createAssistantMessage(
        `ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Bor æ™ºèƒ½ä½“ä¸­æ¢ï¼\n\n**ç³»ç»ŸçŠ¶æ€ï¼š**\n- Ollama æœåŠ¡ï¼šâŒ æœªè¿æ¥\n\n**å¼€å§‹ä½¿ç”¨ï¼š**\n1. è¯·å…ˆå®‰è£… Ollamaï¼šhttps://ollama.ai\n2. å¯åŠ¨ Ollama æœåŠ¡\n3. æ‹‰å–ä¸€ä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚ï¼š\`ollama pull llama2\`\n4. ç„¶åè¯´"æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"é‡æ–°æ£€æµ‹\n\næˆ–è€…è¯´"é…ç½® LLM"æ¥è®¾ç½®å…¶ä»–æ¨¡å‹æä¾›å•†ã€‚`
      )
    } else if (systemStatus.value.availableModels.length === 0) {
      // Ollama å·²è¿æ¥ä½†æ²¡æœ‰æ¨¡å‹
      welcomeMessage = MessageFactory.createAssistantMessage(
        `ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Bor æ™ºèƒ½ä½“ä¸­æ¢ï¼\n\n**ç³»ç»ŸçŠ¶æ€ï¼š**\n- Ollama æœåŠ¡ï¼šâœ… å·²è¿æ¥\n- å¯ç”¨æ¨¡å‹ï¼šâŒ æš‚æ— \n\n**å¼€å§‹ä½¿ç”¨ï¼š**\nè¯·å…ˆæ‹‰å–ä¸€ä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚ï¼š\n\`\`\`bash\nollama pull llama2\n# æˆ–è€…\nollama pull qwen:7b\n\`\`\`\n\nç„¶åè¯´"åˆ·æ–°æ¨¡å‹åˆ—è¡¨"æ¥é‡æ–°æ£€æµ‹ã€‚`
      )
    } else {
      // ä¸€åˆ‡æ­£å¸¸
      welcomeMessage = MessageFactory.createAssistantMessage(
        `ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ Bor æ™ºèƒ½ä½“ä¸­æ¢ï¼\n\n**å½“å‰çŠ¶æ€ï¼š**\n- Ollama æœåŠ¡ï¼šâœ… å·²è¿æ¥\n- å½“å‰æ¨¡å‹ï¼š${systemStatus.value.currentModel}\n- å¯ç”¨æ¨¡å‹ï¼š${systemStatus.value.availableModels.length} ä¸ª\n\næ‚¨å¯ä»¥ç›´æ¥å¼€å§‹å¯¹è¯ï¼Œæˆ–è¯´"æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ã€‚`
      )
    }
    
    messages.value.push(welcomeMessage)
    
    // æ›´æ–°å»ºè®®åˆ—è¡¨
    suggestions.value = getSuggestions()
  }
})
</script>