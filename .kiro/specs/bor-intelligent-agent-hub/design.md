# Bor æ™ºèƒ½ä½“ä¸­æ¢ - è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

Bor æ™ºèƒ½ä½“ä¸­æ¢é‡‡ç”¨é©å‘½æ€§çš„"å¯¹è¯å³ç•Œé¢"è®¾è®¡ç†å¿µï¼Œæ‘’å¼ƒä¼ ç»Ÿçš„èœå•ã€æŒ‰é’®ã€ä¾§è¾¹æ ç­‰å¤æ‚ç•Œé¢å…ƒç´ ï¼Œé€šè¿‡çº¯å¯¹è¯äº¤äº’å®ç°æ‰€æœ‰åŠŸèƒ½æ§åˆ¶ã€‚ç”¨æˆ·åªéœ€è¦é€šè¿‡è‡ªç„¶è¯­è¨€å³å¯å®Œæˆ LLM é…ç½®ã€çŸ¥è¯†åº“ç®¡ç†ã€å·¥ä½œæµåˆ›å»ºã€ç³»ç»Ÿè®¾ç½®ç­‰æ‰€æœ‰æ“ä½œï¼Œæ‰“é€ æç®€è€Œå¼ºå¤§çš„æ™ºèƒ½åŠ©æ‰‹ä½“éªŒã€‚

ç³»ç»Ÿé‡‡ç”¨ç°ä»£åŒ–çš„ Web-First æ¶æ„ï¼Œé€šè¿‡é›†æˆæˆç†Ÿçš„å¼€æºç»„ä»¶æ„å»ºä¸€ä¸ªç‘å£«å†›åˆ€å¼çš„ä¸ªäººæ™ºèƒ½åŠ©æ‰‹å¹³å°ï¼Œæœ€ç»ˆé€šè¿‡ Electron ç­‰æŠ€æœ¯æ‰“åŒ…ä¸ºæ¡Œé¢åº”ç”¨ã€‚

## æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "å‰ç«¯å±‚ (Frontend Layer)"
        UI[Appleé£æ ¼UIç•Œé¢]
        Chat[Chatbot UI Core]
        Workflow[å·¥ä½œæµç¼–è¾‘å™¨]
        Settings[è®¾ç½®ç®¡ç†]
    end
    
    subgraph "ä¸šåŠ¡é€»è¾‘å±‚ (Business Logic Layer)"
        LLM[LLMç®¡ç†å™¨]
        RAG[RAGçŸ¥è¯†åº“]
        MCP[MCPè°ƒåº¦å™¨]
        WF[å·¥ä½œæµå¼•æ“]
        Plugin[æ’ä»¶ç®¡ç†å™¨]
    end
    
    subgraph "æ•°æ®å±‚ (Data Layer)"
        LocalDB[(æœ¬åœ°æ•°æ®åº“)]
        VectorDB[(å‘é‡æ•°æ®åº“)]
        FileStore[(æ–‡ä»¶å­˜å‚¨)]
        Config[(é…ç½®å­˜å‚¨)]
    end
    
    subgraph "å¤–éƒ¨æœåŠ¡ (External Services)"
        OpenAI[OpenAI API]
        Claude[Anthropic API]
        Local[æœ¬åœ°æ¨¡å‹]
        MCPServers[MCPæœåŠ¡å™¨]
    end
    
    UI --> Chat
    UI --> Workflow
    UI --> Settings
    
    Chat --> LLM
    Chat --> RAG
    Chat --> MCP
    Chat --> WF
    
    LLM --> OpenAI
    LLM --> Claude
    LLM --> Local
    
    RAG --> VectorDB
    MCP --> MCPServers
    WF --> Plugin
    
    LLM --> LocalDB
    RAG --> FileStore
    MCP --> Config
    WF --> Config
```

### æŠ€æœ¯æ ˆé€‰æ‹©

#### å‰ç«¯æŠ€æœ¯æ ˆ
- **æ¡†æ¶**: Vue 3 + TypeScript + Vite
- **UI åŸºç¡€**: Chatbot UI (å®šåˆ¶åŒ–)
- **æ ·å¼**: Tailwind CSS + Apple Design System
- **çŠ¶æ€ç®¡ç†**: Pinia
- **è·¯ç”±**: Vue Router
- **æ¡Œé¢åŒ–**: Electron

#### åç«¯æŠ€æœ¯æ ˆ
- **è¿è¡Œæ—¶**: Node.js
- **æ¡†æ¶**: Express.js / Fastify
- **æ•°æ®åº“**: SQLite (æœ¬åœ°) + Chroma (å‘é‡)
- **æ–‡ä»¶å¤„ç†**: Multer + PDF.js
- **è¿›ç¨‹ç®¡ç†**: PM2

#### é›†æˆç»„ä»¶
- **LLM ç®¡ç†**: LLMChat Core
- **MCP åè®®**: @modelcontextprotocol/sdk
- **å·¥ä½œæµå¼•æ“**: è‡ªç ” + Temporal (å¯é€‰)

## ç»„ä»¶è®¾è®¡

### 1. Apple é£æ ¼ UI ç»„ä»¶

#### è®¾è®¡ç³»ç»Ÿ
```typescript
// src/design-system/tokens.ts
export const DesignTokens = {
  colors: {
    primary: {
      blue: '#007AFF',
      indigo: '#5856D6',
      purple: '#AF52DE',
    },
    neutral: {
      white: '#FFFFFF',
      gray100: '#F2F2F7',
      gray200: '#E5E5EA',
      gray300: '#D1D1D6',
      gray400: '#C7C7CC',
      gray500: '#AEAEB2',
      gray600: '#8E8E93',
      black: '#000000',
    },
    semantic: {
      success: '#34C759',
      warning: '#FF9500',
      error: '#FF3B30',
    }
  },
  
  spacing: {
    xs: '4px',
    sm: '8px',
    md: '16px',
    lg: '24px',
    xl: '32px',
    xxl: '48px',
  },
  
  borderRadius: {
    sm: '6px',
    md: '12px',
    lg: '16px',
    xl: '20px',
  },
  
  typography: {
    fontFamily: {
      system: '-apple-system, BlinkMacSystemFont, "SF Pro Display", sans-serif',
      mono: '"SF Mono", Monaco, "Cascadia Code", monospace',
    },
    fontSize: {
      xs: '12px',
      sm: '14px',
      base: '16px',
      lg: '18px',
      xl: '20px',
      '2xl': '24px',
      '3xl': '30px',
    }
  },
  
  effects: {
    glassmorphism: {
      background: 'rgba(255, 255, 255, 0.8)',
      backdropFilter: 'blur(20px)',
      border: '1px solid rgba(255, 255, 255, 0.2)',
    },
    shadow: {
      sm: '0 1px 3px rgba(0, 0, 0, 0.1)',
      md: '0 4px 6px rgba(0, 0, 0, 0.1)',
      lg: '0 10px 15px rgba(0, 0, 0, 0.1)',
    }
  }
}
```

#### æç®€å¯¹è¯ç•Œé¢ç»„ä»¶
```vue
<!-- src/components/ChatInterface.vue -->
<template>
  <div class="chat-interface-minimal">
    <!-- çº¯å‡€çš„æ¶ˆæ¯åˆ—è¡¨ - æ— è¾¹æ¡†ã€æ— è£…é¥° -->
    <div class="messages-container" ref="messagesContainer">
      <ChatMessage
        v-for="message in messages"
        :key="message.id"
        :message="message"
        :is-streaming="message.id === streamingMessageId"
        :show-system-controls="message.type === 'system-config'"
      />
    </div>
    
    <!-- æç®€è¾“å…¥åŒºåŸŸ - æ— æŒ‰é’®ã€æ— èœå• -->
    <div class="input-area-minimal">
      <ChatInput
        v-model="inputText"
        :is-loading="isLoading"
        :placeholder="getContextualPlaceholder()"
        @send="handleIntelligentSend"
        @file-drop="handleFileDrop"
        @voice-input="handleVoiceInput"
      />
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, computed, onMounted } from 'vue'
import { useChatStore } from '@/stores/chat'
import { useLLMStore } from '@/stores/llm'
import ChatMessage from './ChatMessage.vue'
import ChatInput from './ChatInput.vue'

const chatStore = useChatStore()
const llmStore = useLLMStore()

const inputText = ref('')
const isLoading = ref(false)
const streamingMessageId = ref<string | null>(null)

const messages = computed(() => chatStore.currentConversation?.messages || [])
const availableTools = computed(() => chatStore.availableTools)

const handleIntelligentSend = async (content: string, attachments?: File[]) => {
  if (!content.trim() && !attachments?.length) return
  
  isLoading.value = true
  
  try {
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage = await chatStore.addMessage({
      role: 'user',
      content,
      attachments
    })
    
    // æ™ºèƒ½æ„å›¾è¯†åˆ« - é€šè¿‡å¯¹è¯æ§åˆ¶æ‰€æœ‰åŠŸèƒ½
    const intent = await intentRecognizer.analyze(content)
    
    switch (intent.type) {
      case 'system_config':
        await handleSystemConfig(content, intent.params)
        break
      case 'llm_management':
        await handleLLMManagement(content, intent.params)
        break
      case 'knowledge_base':
        await handleKnowledgeBase(content, intent.params, attachments)
        break
      case 'workflow_creation':
        await handleWorkflowRequest(content)
        break
      case 'mcp_management':
        await handleMCPManagement(content, intent.params)
        break
      default:
        await handleChatRequest(userMessage)
    }
  } catch (error) {
    console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error)
  } finally {
    isLoading.value = false
    inputText.value = ''
  }
}

const isWorkflowRequest = (content: string): boolean => {
  const workflowKeywords = ['åˆ›å»ºå·¥ä½œæµ', 'è‡ªåŠ¨åŒ–', 'å®šæ—¶ä»»åŠ¡', 'æ‰¹å¤„ç†', 'è„šæœ¬']
  return workflowKeywords.some(keyword => content.includes(keyword))
}

const handleWorkflowRequest = async (content: string) => {
  // è°ƒç”¨å·¥ä½œæµç”ŸæˆæœåŠ¡
  const workflowService = useWorkflowService()
  const generatedWorkflow = await workflowService.generateFromDescription(content)
  
  // æ˜¾ç¤ºç”Ÿæˆçš„å·¥ä½œæµä¾›ç”¨æˆ·ç¡®è®¤
  await chatStore.addMessage({
    role: 'assistant',
    content: `æˆ‘ä¸ºæ‚¨ç”Ÿæˆäº†ä»¥ä¸‹å·¥ä½œæµï¼Œè¯·ç¡®è®¤æ˜¯å¦ç¬¦åˆæ‚¨çš„éœ€æ±‚ï¼š\n\n\`\`\`yaml\n${generatedWorkflow}\n\`\`\``,
    metadata: {
      type: 'workflow',
      workflow: generatedWorkflow
    }
  })
}
</script>
```

### 2. LLM ç®¡ç†ç³»ç»Ÿ

#### LLM ç®¡ç†å™¨æ¶æ„
```typescript
// src/services/llm/LLMManager.ts
import { LLMProvider, ChatRequest, ChatResponse } from './types'
import { OpenAIProvider } from './providers/OpenAIProvider'
import { AnthropicProvider } from './providers/AnthropicProvider'
import { OllamaProvider } from './providers/OllamaProvider'

export class LLMManager {
  private providers: Map<string, LLMProvider> = new Map()
  private defaultProvider: string = 'openai'
  
  constructor() {
    this.initializeProviders()
  }
  
  private initializeProviders() {
    // æ³¨å†Œå„ç§ LLM æä¾›å•†
    this.providers.set('openai', new OpenAIProvider())
    this.providers.set('anthropic', new AnthropicProvider())
    this.providers.set('ollama', new OllamaProvider())
  }
  
  async chat(request: ChatRequest): Promise<ChatResponse> {
    const provider = this.selectProvider(request)
    
    // é›†æˆ RAG å¢å¼º
    const enhancedRequest = await this.enhanceWithRAG(request)
    
    // æ‰§è¡Œå¯¹è¯
    return await provider.chat(enhancedRequest)
  }
  
  private selectProvider(request: ChatRequest): LLMProvider {
    // æ™ºèƒ½è·¯ç”±é€»è¾‘
    if (request.preferredModel) {
      const provider = this.providers.get(request.preferredModel)
      if (provider) return provider
    }
    
    // æ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
    if (request.requiresCodeGeneration) {
      return this.providers.get('anthropic') || this.providers.get('openai')!
    }
    
    if (request.requiresPrivacy) {
      return this.providers.get('ollama')!
    }
    
    return this.providers.get(this.defaultProvider)!
  }
  
  private async enhanceWithRAG(request: ChatRequest): Promise<ChatRequest> {
    const ragService = useRAGService()
    const relevantDocs = await ragService.search(request.content)
    
    if (relevantDocs.length > 0) {
      const context = relevantDocs.map(doc => doc.content).join('\n\n')
      request.content = `åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\n${context}\n\né—®é¢˜ï¼š${request.content}`
    }
    
    return request
  }
}
```

#### å¤šä¾›åº”å•†æä¾›å•†å®ç°
```typescript
// src/services/llm/providers/ProviderFactory.ts
import { LLMProvider } from '../types'
import { OllamaProvider } from './OllamaProvider'
import { OpenRouterProvider } from './OpenRouterProvider'
import { OpenAIProvider } from './OpenAIProvider'
import { OpenAICompatibleProvider } from './OpenAICompatibleProvider'
import { ZhipuProvider } from './ZhipuProvider'
import { GeminiProvider } from './GeminiProvider'

export class ProviderFactory {
  static createProvider(config: ProviderConfig): LLMProvider {
    switch (config.type) {
      case 'ollama':
        return new OllamaProvider(config)
      case 'openrouter':
        return new OpenRouterProvider(config)
      case 'openai':
        return new OpenAIProvider(config)
      case 'openai-compatible':
        return new OpenAICompatibleProvider(config)
      case 'zhipu':
        return new ZhipuProvider(config)
      case 'gemini':
        return new GeminiProvider(config)
      default:
        throw new Error(`ä¸æ”¯æŒçš„ä¾›åº”å•†ç±»å‹: ${config.type}`)
    }
  }
}

// src/services/llm/providers/OllamaProvider.ts
export class OllamaProvider implements LLMProvider {
  private baseURL: string
  
  constructor(config: ProviderConfig) {
    this.baseURL = config.baseURL || 'http://localhost:11434'
  }
  
  async getAvailableModels(): Promise<string[]> {
    const response = await fetch(`${this.baseURL}/api/tags`)
    const data = await response.json()
    return data.models.map((model: any) => model.name)
  }
  
  async chat(request: ChatRequest): Promise<ChatResponse> {
    const response = await fetch(`${this.baseURL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: request.model,
        messages: request.messages,
        stream: true
      })
    })
    
    return this.handleStreamResponse(response)
  }
}

// src/services/llm/providers/OpenRouterProvider.ts
export class OpenRouterProvider implements LLMProvider {
  private client: OpenAI
  
  constructor(config: ProviderConfig) {
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: 'https://openrouter.ai/api/v1',
      defaultHeaders: {
        'HTTP-Referer': 'https://bor-ai.app',
        'X-Title': 'Boræ™ºèƒ½ä½“ä¸­æ¢'
      }
    })
  }
  
  async getAvailableModels(): Promise<string[]> {
    const response = await fetch('https://openrouter.ai/api/v1/models', {
      headers: {
        'Authorization': `Bearer ${this.config.apiKey}`
      }
    })
    const data = await response.json()
    return data.data.map((model: any) => model.id)
  }
}

// src/services/llm/providers/ZhipuProvider.ts
export class ZhipuProvider implements LLMProvider {
  private apiKey: string
  private baseURL: string = 'https://open.bigmodel.cn/api/paas/v4'
  
  constructor(config: ProviderConfig) {
    this.apiKey = config.apiKey
  }
  
  async chat(request: ChatRequest): Promise<ChatResponse> {
    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: request.model || 'glm-4',
        messages: request.messages,
        stream: true
      })
    })
    
    return this.handleStreamResponse(response)
  }
  
  async getAvailableModels(): Promise<string[]> {
    return ['glm-4', 'glm-4v', 'glm-3-turbo']
  }
}

// src/services/llm/providers/GeminiProvider.ts
export class GeminiProvider implements LLMProvider {
  private apiKey: string
  private baseURL: string = 'https://generativelanguage.googleapis.com/v1beta'
  
  constructor(config: ProviderConfig) {
    this.apiKey = config.apiKey
  }
  
  async chat(request: ChatRequest): Promise<ChatResponse> {
    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸ºGeminiæ ¼å¼
    const geminiMessages = this.convertToGeminiFormat(request.messages)
    
    const response = await fetch(`${this.baseURL}/models/${request.model}:streamGenerateContent?key=${this.apiKey}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: geminiMessages,
        generationConfig: {
          temperature: request.temperature || 0.7
        }
      })
    })
    
    return this.handleGeminiStreamResponse(response)
  }
  
  async getAvailableModels(): Promise<string[]> {
    return ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro-vision']
  }
}
  
  async chat(request: ChatRequest): Promise<ChatResponse> {
    const stream = await this.client.chat.completions.create({
      model: request.model || 'gpt-4',
      messages: request.messages,
      stream: true,
      tools: request.availableTools?.map(tool => ({
        type: 'function',
        function: {
          name: tool.name,
          description: tool.description,
          parameters: tool.parameters
        }
      }))
    })
    
    return this.handleStreamResponse(stream)
  }
  
  private async handleStreamResponse(stream: any): Promise<ChatResponse> {
    let content = ''
    const toolCalls: any[] = []
    
    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta
      
      if (delta?.content) {
        content += delta.content
        // å‘é€æµå¼æ›´æ–°äº‹ä»¶
        this.emitStreamUpdate(delta.content)
      }
      
      if (delta?.tool_calls) {
        toolCalls.push(...delta.tool_calls)
      }
    }
    
    return {
      content,
      toolCalls,
      usage: stream.usage
    }
  }
  
  private emitStreamUpdate(content: string) {
    // å‘é€å®æ—¶æ›´æ–°äº‹ä»¶ç»™å‰ç«¯
    window.electronAPI?.sendStreamUpdate(content)
  }
}
```

### 3. RAG çŸ¥è¯†åº“ç³»ç»Ÿ

#### çŸ¥è¯†åº“ç®¡ç†å™¨
```typescript
// src/services/rag/RAGService.ts
import { ChromaClient } from 'chromadb'
import { Document, SearchResult } from './types'

export class RAGService {
  private chromaClient: ChromaClient
  private collection: any
  
  constructor() {
    this.chromaClient = new ChromaClient({
      path: 'http://localhost:8000'
    })
    this.initializeCollection()
  }
  
  private async initializeCollection() {
    this.collection = await this.chromaClient.getOrCreateCollection({
      name: 'bor-knowledge-base',
      metadata: { description: 'Boræ™ºèƒ½ä½“ä¸­æ¢çŸ¥è¯†åº“' }
    })
  }
  
  async addDocument(document: Document): Promise<void> {
    // æ–‡æ¡£åˆ†å—
    const chunks = this.chunkDocument(document)
    
    // ç”Ÿæˆå‘é‡åµŒå…¥
    const embeddings = await this.generateEmbeddings(chunks)
    
    // å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
    await this.collection.add({
      ids: chunks.map((_, index) => `${document.id}_chunk_${index}`),
      embeddings,
      documents: chunks.map(chunk => chunk.content),
      metadatas: chunks.map(chunk => ({
        documentId: document.id,
        title: document.title,
        type: document.type,
        chunkIndex: chunk.index
      }))
    })
  }
  
  async search(query: string, limit: number = 5): Promise<SearchResult[]> {
    const queryEmbedding = await this.generateEmbeddings([query])
    
    const results = await this.collection.query({
      queryEmbeddings: queryEmbedding,
      nResults: limit,
      include: ['documents', 'metadatas', 'distances']
    })
    
    return results.documents[0].map((doc: string, index: number) => ({
      content: doc,
      metadata: results.metadatas[0][index],
      similarity: 1 - results.distances[0][index]
    }))
  }
  
  private chunkDocument(document: Document): DocumentChunk[] {
    // æ™ºèƒ½åˆ†å—ç®—æ³•
    const maxChunkSize = 1000
    const overlap = 200
    
    const chunks: DocumentChunk[] = []
    let startIndex = 0
    
    while (startIndex < document.content.length) {
      const endIndex = Math.min(startIndex + maxChunkSize, document.content.length)
      const chunkContent = document.content.slice(startIndex, endIndex)
      
      chunks.push({
        content: chunkContent,
        index: chunks.length,
        startIndex,
        endIndex
      })
      
      startIndex = endIndex - overlap
    }
    
    return chunks
  }
  
  private async generateEmbeddings(texts: string[]): Promise<number[][]> {
    // ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹æˆ–è°ƒç”¨ API
    const response = await fetch('/api/embeddings', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ texts })
    })
    
    const data = await response.json()
    return data.embeddings
  }
}
```

### 4. MCP è°ƒåº¦ç³»ç»Ÿ

#### MCP ç®¡ç†å™¨
```typescript
// src/services/mcp/MCPManager.ts
import { Client, Server, Transport } from '@modelcontextprotocol/sdk'
import { MCPServerConfig, ToolInfo } from './types'

export class MCPManager {
  private servers: Map<string, MCPServerInstance> = new Map()
  private client: Client
  
  constructor() {
    this.client = new Client({
      name: 'bor-intelligent-agent',
      version: '1.0.0'
    })
    
    this.loadServerConfigs()
  }
  
  private async loadServerConfigs() {
    const configs = await this.getServerConfigs()
    
    for (const config of configs) {
      await this.registerServer(config)
    }
  }
  
  async registerServer(config: MCPServerConfig): Promise<void> {
    try {
      const transport = this.createTransport(config)
      const server = new MCPServerInstance(config.name, transport)
      
      await server.connect()
      this.servers.set(config.name, server)
      
      console.log(`MCPæœåŠ¡å™¨ ${config.name} æ³¨å†ŒæˆåŠŸ`)
    } catch (error) {
      console.error(`æ³¨å†ŒMCPæœåŠ¡å™¨ ${config.name} å¤±è´¥:`, error)
    }
  }
  
  async callTool(serverName: string, toolName: string, args: any): Promise<any> {
    const server = this.servers.get(serverName)
    if (!server) {
      throw new Error(`MCPæœåŠ¡å™¨ ${serverName} æœªæ‰¾åˆ°`)
    }
    
    return await server.callTool(toolName, args)
  }
  
  async listAvailableTools(): Promise<ToolInfo[]> {
    const allTools: ToolInfo[] = []
    
    for (const [serverName, server] of this.servers) {
      try {
        const tools = await server.listTools()
        allTools.push(...tools.map(tool => ({
          ...tool,
          serverName
        })))
      } catch (error) {
        console.error(`è·å–æœåŠ¡å™¨ ${serverName} å·¥å…·åˆ—è¡¨å¤±è´¥:`, error)
      }
    }
    
    return allTools
  }
  
  private createTransport(config: MCPServerConfig): Transport {
    switch (config.transport.type) {
      case 'stdio':
        return new StdioTransport(config.transport.command, config.transport.args)
      case 'websocket':
        return new WebSocketTransport(config.transport.url)
      default:
        throw new Error(`ä¸æ”¯æŒçš„ä¼ è¾“ç±»å‹: ${config.transport.type}`)
    }
  }
  
  private async getServerConfigs(): Promise<MCPServerConfig[]> {
    // ä»é…ç½®æ–‡ä»¶åŠ è½½ MCP æœåŠ¡å™¨é…ç½®
    const configPath = path.join(app.getPath('userData'), 'mcp-servers.json')
    
    if (!fs.existsSync(configPath)) {
      // åˆ›å»ºé»˜è®¤é…ç½®
      const defaultConfigs = this.getDefaultServerConfigs()
      await fs.promises.writeFile(configPath, JSON.stringify(defaultConfigs, null, 2))
      return defaultConfigs
    }
    
    const configData = await fs.promises.readFile(configPath, 'utf-8')
    return JSON.parse(configData)
  }
  
  private getDefaultServerConfigs(): MCPServerConfig[] {
    return [
      {
        name: 'filesystem',
        transport: {
          type: 'stdio',
          command: 'npx',
          args: ['@modelcontextprotocol/server-filesystem', './workspace']
        },
        autoStart: true
      },
      {
        name: 'brave-search',
        transport: {
          type: 'stdio',
          command: 'npx',
          args: ['@modelcontextprotocol/server-brave-search']
        },
        autoStart: false,
        env: {
          BRAVE_API_KEY: process.env.BRAVE_API_KEY || ''
        }
      }
    ]
  }
}
```

### 5. å¯¹è¯å¼å·¥ä½œæµç³»ç»Ÿ

#### å·¥ä½œæµç”Ÿæˆå™¨
```typescript
// src/services/workflow/WorkflowGenerator.ts
export class WorkflowGenerator {
  private llmManager: LLMManager
  
  constructor(llmManager: LLMManager) {
    this.llmManager = llmManager
  }
  
  async generateFromDescription(description: string): Promise<WorkflowDefinition> {
    const prompt = this.buildGenerationPrompt(description)
    
    const response = await this.llmManager.chat({
      content: prompt,
      model: 'gpt-4',
      temperature: 0.1
    })
    
    // è§£æç”Ÿæˆçš„å·¥ä½œæµ
    const workflowYaml = this.extractYamlFromResponse(response.content)
    const workflow = this.parseWorkflow(workflowYaml)
    
    // éªŒè¯å·¥ä½œæµ
    this.validateWorkflow(workflow)
    
    return workflow
  }
  
  private buildGenerationPrompt(description: string): string {
    return `
ä½œä¸ºä¸€ä¸ªå·¥ä½œæµä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æè¿°ç”Ÿæˆä¸€ä¸ªYAMLæ ¼å¼çš„å·¥ä½œæµå®šä¹‰ã€‚

ç”¨æˆ·æè¿°ï¼š${description}

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«ä»¥ä¸‹ç»“æ„çš„å·¥ä½œæµï¼š
- name: å·¥ä½œæµåç§°
- description: å·¥ä½œæµæè¿°
- triggers: è§¦å‘æ¡ä»¶
- steps: æ‰§è¡Œæ­¥éª¤
- error_handling: é”™è¯¯å¤„ç†

æ¯ä¸ªæ­¥éª¤åº”è¯¥åŒ…å«ï¼š
- name: æ­¥éª¤åç§°
- type: æ­¥éª¤ç±»å‹ (llm_call, mcp_tool, condition, loopç­‰)
- config: æ­¥éª¤é…ç½®
- next: ä¸‹ä¸€æ­¥éª¤

è¯·ç¡®ä¿ç”Ÿæˆçš„å·¥ä½œæµæ˜¯å¯æ‰§è¡Œçš„ï¼Œå¹¶ä¸”ç¬¦åˆYAMLè¯­æ³•è§„èŒƒã€‚

ç¤ºä¾‹æ ¼å¼ï¼š
\`\`\`yaml
name: "ç¤ºä¾‹å·¥ä½œæµ"
description: "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å·¥ä½œæµ"
triggers:
  - type: "manual"
steps:
  - name: "æ­¥éª¤1"
    type: "llm_call"
    config:
      model: "gpt-4"
      prompt: "åˆ†æç”¨æˆ·è¾“å…¥"
    next: "æ­¥éª¤2"
\`\`\`
`
  }
  
  async optimizeWorkflow(workflow: WorkflowDefinition, feedback: string): Promise<WorkflowDefinition> {
    const prompt = `
è¯·æ ¹æ®ç”¨æˆ·åé¦ˆä¼˜åŒ–ä»¥ä¸‹å·¥ä½œæµï¼š

å½“å‰å·¥ä½œæµï¼š
\`\`\`yaml
${this.workflowToYaml(workflow)}
\`\`\`

ç”¨æˆ·åé¦ˆï¼š${feedback}

è¯·æä¾›ä¼˜åŒ–åçš„å·¥ä½œæµå®šä¹‰ã€‚
`
    
    const response = await this.llmManager.chat({
      content: prompt,
      model: 'gpt-4',
      temperature: 0.1
    })
    
    const optimizedYaml = this.extractYamlFromResponse(response.content)
    return this.parseWorkflow(optimizedYaml)
  }
}
```

#### å·¥ä½œæµæ‰§è¡Œå¼•æ“
```typescript
// src/services/workflow/WorkflowEngine.ts
export class WorkflowEngine {
  private mcpManager: MCPManager
  private llmManager: LLMManager
  private executionContext: Map<string, any> = new Map()
  
  async executeWorkflow(workflow: WorkflowDefinition, input?: any): Promise<WorkflowResult> {
    const executionId = this.generateExecutionId()
    const context = new WorkflowExecutionContext(executionId, workflow, input)
    
    try {
      // åˆå§‹åŒ–æ‰§è¡Œä¸Šä¸‹æ–‡
      this.executionContext.set(executionId, context)
      
      // æ‰§è¡Œå·¥ä½œæµæ­¥éª¤
      const result = await this.executeSteps(workflow.steps, context)
      
      return {
        executionId,
        status: 'completed',
        result,
        executionTime: Date.now() - context.startTime
      }
    } catch (error) {
      return {
        executionId,
        status: 'failed',
        error: error.message,
        executionTime: Date.now() - context.startTime
      }
    } finally {
      this.executionContext.delete(executionId)
    }
  }
  
  private async executeSteps(steps: WorkflowStep[], context: WorkflowExecutionContext): Promise<any> {
    let currentStep = steps[0]
    let result: any = context.input
    
    while (currentStep) {
      try {
        // æ›´æ–°æ‰§è¡ŒçŠ¶æ€
        context.currentStep = currentStep.name
        this.emitExecutionUpdate(context)
        
        // æ‰§è¡Œå½“å‰æ­¥éª¤
        result = await this.executeStep(currentStep, result, context)
        
        // ç¡®å®šä¸‹ä¸€æ­¥
        currentStep = this.getNextStep(currentStep, result, steps)
        
      } catch (error) {
        // é”™è¯¯å¤„ç†
        if (currentStep.errorHandling) {
          result = await this.handleStepError(error, currentStep, context)
          currentStep = this.getNextStep(currentStep, result, steps)
        } else {
          throw error
        }
      }
    }
    
    return result
  }
  
  private async executeStep(step: WorkflowStep, input: any, context: WorkflowExecutionContext): Promise<any> {
    switch (step.type) {
      case 'llm_call':
        return await this.executeLLMStep(step, input, context)
      
      case 'mcp_tool':
        return await this.executeMCPStep(step, input, context)
      
      case 'condition':
        return await this.executeConditionStep(step, input, context)
      
      case 'loop':
        return await this.executeLoopStep(step, input, context)
      
      case 'parallel':
        return await this.executeParallelStep(step, input, context)
      
      default:
        throw new Error(`ä¸æ”¯æŒçš„æ­¥éª¤ç±»å‹: ${step.type}`)
    }
  }
  
  private async executeLLMStep(step: WorkflowStep, input: any, context: WorkflowExecutionContext): Promise<any> {
    const config = step.config as LLMStepConfig
    
    const response = await this.llmManager.chat({
      content: this.interpolateTemplate(config.prompt, { input, context: context.variables }),
      model: config.model || 'gpt-4',
      temperature: config.temperature || 0.7
    })
    
    return response.content
  }
  
  private async executeMCPStep(step: WorkflowStep, input: any, context: WorkflowExecutionContext): Promise<any> {
    const config = step.config as MCPStepConfig
    
    const args = this.interpolateObject(config.args, { input, context: context.variables })
    
    return await this.mcpManager.callTool(config.server, config.tool, args)
  }
}
```

## æ•°æ®æ¨¡å‹

### æ ¸å¿ƒæ•°æ®ç»“æ„
```typescript
// src/types/core.ts

export interface Message {
  id: string
  role: 'user' | 'assistant' | 'system'
  content: string
  attachments?: Attachment[]
  metadata?: MessageMetadata
  timestamp: number
}

export interface Conversation {
  id: string
  title: string
  messages: Message[]
  model?: string
  createdAt: number
  updatedAt: number
}

export interface Document {
  id: string
  title: string
  content: string
  type: 'pdf' | 'docx' | 'txt' | 'md'
  size: number
  uploadedAt: number
  vectorized: boolean
}

export interface WorkflowDefinition {
  id: string
  name: string
  description: string
  version: string
  triggers: WorkflowTrigger[]
  steps: WorkflowStep[]
  variables?: Record<string, any>
  createdAt: number
  updatedAt: number
}

export interface MCPServerConfig {
  name: string
  transport: {
    type: 'stdio' | 'websocket'
    command?: string
    args?: string[]
    url?: string
  }
  env?: Record<string, string>
  autoStart: boolean
  healthCheck?: {
    interval: number
    timeout: number
  }
}
```

## é”™è¯¯å¤„ç†

### ç»Ÿä¸€é”™è¯¯å¤„ç†æœºåˆ¶
```typescript
// src/utils/errorHandler.ts
export class ErrorHandler {
  static handle(error: Error, context?: string): void {
    // è®°å½•é”™è¯¯æ—¥å¿—
    console.error(`[${context || 'Unknown'}] ${error.message}`, error.stack)
    
    // å‘é€é”™è¯¯æŠ¥å‘Š
    this.reportError(error, context)
    
    // æ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
    this.showUserError(error)
  }
  
  private static reportError(error: Error, context?: string): void {
    // å‘é€åˆ°é”™è¯¯ç›‘æ§æœåŠ¡
    if (process.env.NODE_ENV === 'production') {
      // Sentry.captureException(error, { tags: { context } })
    }
  }
  
  private static showUserError(error: Error): void {
    const userMessage = this.getUserFriendlyMessage(error)
    
    // é€šè¿‡ UI æ˜¾ç¤ºé”™è¯¯
    window.electronAPI?.showError(userMessage)
  }
  
  private static getUserFriendlyMessage(error: Error): string {
    if (error.message.includes('API key')) {
      return 'è¯·æ£€æŸ¥æ‚¨çš„ API å¯†é’¥é…ç½®'
    }
    
    if (error.message.includes('network')) {
      return 'ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®'
    }
    
    if (error.message.includes('MCP')) {
      return 'MCP æœåŠ¡å™¨è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€'
    }
    
    return 'æ“ä½œå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•'
  }
}
```

## æµ‹è¯•ç­–ç•¥

### æµ‹è¯•æ¶æ„
```typescript
// tests/unit/services/LLMManager.test.ts
import { describe, it, expect, vi } from 'vitest'
import { LLMManager } from '@/services/llm/LLMManager'

describe('LLMManager', () => {
  it('åº”è¯¥èƒ½å¤Ÿæ­£ç¡®è·¯ç”±åˆ°åˆé€‚çš„æ¨¡å‹æä¾›å•†', async () => {
    const llmManager = new LLMManager()
    
    const request = {
      content: 'è¯·å¸®æˆ‘å†™ä¸€æ®µä»£ç ',
      requiresCodeGeneration: true
    }
    
    const response = await llmManager.chat(request)
    
    expect(response).toBeDefined()
    expect(response.content).toContain('ä»£ç ')
  })
  
  it('åº”è¯¥èƒ½å¤Ÿé›†æˆ RAG å¢å¼ºè¯·æ±‚', async () => {
    const llmManager = new LLMManager()
    
    // Mock RAG æœåŠ¡
    vi.mock('@/services/rag/RAGService', () => ({
      search: vi.fn().mockResolvedValue([
        { content: 'ç›¸å…³æ–‡æ¡£å†…å®¹', similarity: 0.8 }
      ])
    }))
    
    const request = {
      content: 'å…³äºé¡¹ç›®çš„é—®é¢˜'
    }
    
    const response = await llmManager.chat(request)
    
    expect(response.content).toBeDefined()
  })
})
```

### é›†æˆæµ‹è¯•
```typescript
// tests/integration/workflow.test.ts
import { describe, it, expect } from 'vitest'
import { WorkflowEngine } from '@/services/workflow/WorkflowEngine'
import { WorkflowGenerator } from '@/services/workflow/WorkflowGenerator'

describe('å·¥ä½œæµç³»ç»Ÿé›†æˆæµ‹è¯•', () => {
  it('åº”è¯¥èƒ½å¤Ÿä»æè¿°ç”Ÿæˆå¹¶æ‰§è¡Œå·¥ä½œæµ', async () => {
    const generator = new WorkflowGenerator()
    const engine = new WorkflowEngine()
    
    // ç”Ÿæˆå·¥ä½œæµ
    const workflow = await generator.generateFromDescription(
      'åˆ›å»ºä¸€ä¸ªæ¯å¤©å®šæ—¶å‘é€é‚®ä»¶çš„å·¥ä½œæµ'
    )
    
    expect(workflow.name).toBeDefined()
    expect(workflow.steps).toHaveLength.greaterThan(0)
    
    // æ‰§è¡Œå·¥ä½œæµ
    const result = await engine.executeWorkflow(workflow, {
      recipient: 'test@example.com',
      subject: 'æµ‹è¯•é‚®ä»¶'
    })
    
    expect(result.status).toBe('completed')
  })
})
```

è¿™ä¸ªè®¾è®¡æ–‡æ¡£æ¶µç›–äº†ç³»ç»Ÿçš„æ ¸å¿ƒæ¶æ„ã€ä¸»è¦ç»„ä»¶è®¾è®¡ã€æ•°æ®æ¨¡å‹å’Œæµ‹è¯•ç­–ç•¥ã€‚è®¾è®¡å……åˆ†è€ƒè™‘äº†ä½ æå‡ºçš„éœ€æ±‚ï¼Œç‰¹åˆ«æ˜¯ Apple é£æ ¼ç•Œé¢ã€å¯¹è¯å¼å·¥ä½œæµå’Œç³»ç»Ÿæ‰©å±•æ€§ã€‚
### 6
. å¯¹è¯å¼ç³»ç»Ÿæ§åˆ¶

#### æ„å›¾è¯†åˆ«ç³»ç»Ÿ
```typescript
// src/services/intent/IntentRecognizer.ts
export class IntentRecognizer {
  private llmManager: LLMManager
  
  constructor(llmManager: LLMManager) {
    this.llmManager = llmManager
  }
  
  async analyze(userInput: string): Promise<IntentResult> {
    const prompt = this.buildIntentPrompt(userInput)
    
    const response = await this.llmManager.chat({
      content: prompt,
      model: 'gpt-4',
      temperature: 0.1
    })
    
    return this.parseIntentResponse(response.content)
  }
  
  private buildIntentPrompt(userInput: string): string {
    return `
åˆ†æç”¨æˆ·è¾“å…¥çš„æ„å›¾ï¼Œåˆ¤æ–­ç”¨æˆ·æƒ³è¦æ‰§è¡Œä»€ä¹ˆæ“ä½œã€‚

ç”¨æˆ·è¾“å…¥ï¼š${userInput}

è¯·ä»ä»¥ä¸‹æ„å›¾ç±»å‹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€ä¸ªï¼š

1. system_config - ç³»ç»Ÿé…ç½®ç›¸å…³
   ç¤ºä¾‹ï¼šåˆ‡æ¢ä¸»é¢˜ã€ä¿®æ”¹è®¾ç½®ã€è°ƒæ•´ç•Œé¢
   
2. llm_management - LLMæ¨¡å‹ç®¡ç†
   ç¤ºä¾‹ï¼šåˆ‡æ¢æ¨¡å‹ã€é…ç½®LLMã€ç®¡ç†æ¨¡å‹å‚æ•°
   
3. knowledge_base - çŸ¥è¯†åº“ç®¡ç†
   ç¤ºä¾‹ï¼šä¸Šä¼ æ–‡æ¡£ã€æœç´¢çŸ¥è¯†ã€ç®¡ç†æ–‡ä»¶
   
4. workflow_creation - å·¥ä½œæµåˆ›å»º
   ç¤ºä¾‹ï¼šåˆ›å»ºè‡ªåŠ¨åŒ–ä»»åŠ¡ã€å®šæ—¶ä»»åŠ¡ã€æ‰¹å¤„ç†
   
5. mcp_management - MCPå·¥å…·ç®¡ç†
   ç¤ºä¾‹ï¼šæ·»åŠ å·¥å…·ã€é…ç½®æœåŠ¡å™¨ã€è°ƒç”¨å·¥å…·
   
6. general_chat - æ™®é€šå¯¹è¯
   ç¤ºä¾‹ï¼šé—®ç­”ã€é—²èŠã€å’¨è¯¢

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
  "type": "æ„å›¾ç±»å‹",
  "confidence": 0.95,
  "params": {
    "å…·ä½“å‚æ•°": "å‚æ•°å€¼"
  },
  "explanation": "è¯†åˆ«ç†ç”±"
}
`
  }
  
  private parseIntentResponse(response: string): IntentResult {
    try {
      const jsonMatch = response.match(/\{[\s\S]*\}/)
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0])
      }
    } catch (error) {
      console.error('è§£ææ„å›¾å“åº”å¤±è´¥:', error)
    }
    
    // é»˜è®¤è¿”å›æ™®é€šå¯¹è¯æ„å›¾
    return {
      type: 'general_chat',
      confidence: 0.5,
      params: {},
      explanation: 'æ— æ³•è¯†åˆ«å…·ä½“æ„å›¾ï¼Œé»˜è®¤ä¸ºæ™®é€šå¯¹è¯'
    }
  }
}
```

#### å¯¹è¯å¼ç³»ç»Ÿé…ç½®
```typescript
// src/services/config/DialogueConfigManager.ts
export class DialogueConfigManager {
  private configStore: ConfigStore
  private llmManager: LLMManager
  
  async handleSystemConfig(userInput: string, params: any): Promise<string> {
    // è§£æé…ç½®æ„å›¾
    const configIntent = await this.parseConfigIntent(userInput)
    
    switch (configIntent.action) {
      case 'theme_change':
        return await this.handleThemeChange(configIntent.value)
      
      case 'api_key_setup':
        return await this.handleApiKeySetup(configIntent.provider, configIntent.key)
      
      case 'model_switch':
        return await this.handleModelSwitch(configIntent.model)
      
      case 'settings_view':
        return await this.handleSettingsView()
      
      default:
        return await this.handleGenericConfig(userInput)
    }
  }
  
  private async handleThemeChange(theme: 'light' | 'dark' | 'auto'): Promise<string> {
    await this.configStore.updateTheme(theme)
    
    // åº”ç”¨ä¸»é¢˜å˜æ›´
    document.documentElement.setAttribute('data-theme', theme)
    
    return `å·²åˆ‡æ¢åˆ°${theme === 'light' ? 'æµ…è‰²' : theme === 'dark' ? 'æ·±è‰²' : 'è‡ªåŠ¨'}ä¸»é¢˜`
  }
  
  private async handleApiKeySetup(provider: string): Promise<string> {
    // æ‰“å¼€å®‰å…¨çš„é…ç½®é¡µé¢è€Œä¸æ˜¯åœ¨å¯¹è¯ä¸­å¤„ç†æ•æ„Ÿä¿¡æ¯
    await this.openSecureConfigPage('api-keys', { provider })
    
    return `æ­£åœ¨æ‰“å¼€${provider} APIå¯†é’¥é…ç½®é¡µé¢...

ä¸ºäº†ä¿æŠ¤æ‚¨çš„éšç§å’Œå®‰å…¨ï¼ŒAPIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯å°†åœ¨ä¸“é—¨çš„å®‰å…¨é…ç½®é¡µé¢ä¸­å¤„ç†ï¼Œä¸ä¼šåœ¨å¯¹è¯è®°å½•ä¸­ç•™ä¸‹ç—•è¿¹ã€‚`
  }
  
  private async handleModelSwitch(model: string): Promise<string> {
    const availableModels = await this.llmManager.getAvailableModels()
    
    if (!availableModels.includes(model)) {
      const suggestions = availableModels.slice(0, 3).join('ã€')
      return `æ¨¡å‹ ${model} ä¸å¯ç”¨ã€‚å¯ç”¨æ¨¡å‹ï¼š${suggestions}`
    }
    
    await this.configStore.setDefaultModel(model)
    
    return `å·²åˆ‡æ¢åˆ° ${model} æ¨¡å‹`
  }
  
  private async handleSettingsView(): Promise<string> {
    const currentSettings = await this.configStore.getAllSettings()
    
    return `å½“å‰ç³»ç»Ÿè®¾ç½®ï¼š
    
ğŸ¨ ä¸»é¢˜ï¼š${currentSettings.theme}
ğŸ¤– é»˜è®¤æ¨¡å‹ï¼š${currentSettings.defaultModel}
ğŸ”‘ å·²é…ç½®çš„APIï¼š${currentSettings.configuredApis.join('ã€')}
ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£æ•°ï¼š${currentSettings.knowledgeBaseCount}
âš™ï¸ MCPæœåŠ¡å™¨æ•°ï¼š${currentSettings.mcpServerCount}

éœ€è¦ä¿®æ”¹ä»»ä½•è®¾ç½®ï¼Œè¯·ç›´æ¥å‘Šè¯‰æˆ‘ï¼`
  }
}
```

#### å¯¹è¯å¼çŸ¥è¯†åº“ç®¡ç†
```typescript
// src/services/knowledge/DialogueKnowledgeManager.ts
export class DialogueKnowledgeManager {
  private ragService: RAGService
  
  async handleKnowledgeBase(userInput: string, params: any, attachments?: File[]): Promise<string> {
    const action = await this.parseKnowledgeAction(userInput)
    
    switch (action.type) {
      case 'upload_document':
        return await this.handleDocumentUpload(attachments, action.metadata)
      
      case 'search_knowledge':
        return await this.handleKnowledgeSearch(action.query)
      
      case 'list_documents':
        return await this.handleDocumentList()
      
      case 'delete_document':
        return await this.handleDocumentDelete(action.documentId)
      
      default:
        return await this.handleGenericKnowledge(userInput)
    }
  }
  
  private async handleDocumentUpload(files?: File[], metadata?: any): Promise<string> {
    if (!files || files.length === 0) {
      return 'è¯·æ‹–æ‹½æ–‡ä»¶åˆ°å¯¹è¯æ¡†ï¼Œæˆ–è€…è¯´"ä¸Šä¼ æ–‡æ¡£"ç„¶åé€‰æ‹©æ–‡ä»¶'
    }
    
    const results = []
    
    for (const file of files) {
      try {
        const document = await this.processFile(file)
        await this.ragService.addDocument(document)
        results.push(`âœ… ${file.name} ä¸Šä¼ æˆåŠŸ`)
      } catch (error) {
        results.push(`âŒ ${file.name} ä¸Šä¼ å¤±è´¥ï¼š${error.message}`)
      }
    }
    
    return `æ–‡æ¡£ä¸Šä¼ ç»“æœï¼š\n${results.join('\n')}\n\nç°åœ¨æ‚¨å¯ä»¥åŸºäºè¿™äº›æ–‡æ¡£è¿›è¡Œé—®ç­”äº†ï¼`
  }
  
  private async handleKnowledgeSearch(query: string): Promise<string> {
    const results = await this.ragService.search(query, 3)
    
    if (results.length === 0) {
      return 'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ã€‚æ‚¨å¯ä»¥ä¸Šä¼ ç›¸å…³æ–‡æ¡£æ¥ä¸°å¯ŒçŸ¥è¯†åº“ã€‚'
    }
    
    const formattedResults = results.map((result, index) => 
      `${index + 1}. **${result.metadata.title}** (ç›¸ä¼¼åº¦: ${(result.similarity * 100).toFixed(1)}%)\n${result.content.substring(0, 200)}...`
    ).join('\n\n')
    
    return `æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n${formattedResults}`
  }
}
```

#### å¯¹è¯å¼å·¥ä½œæµç®¡ç†
```typescript
// src/services/workflow/DialogueWorkflowManager.ts
export class DialogueWorkflowManager {
  private workflowGenerator: WorkflowGenerator
  private workflowEngine: WorkflowEngine
  
  async handleWorkflowRequest(userInput: string): Promise<string> {
    const workflowIntent = await this.parseWorkflowIntent(userInput)
    
    switch (workflowIntent.action) {
      case 'create':
        return await this.handleWorkflowCreation(workflowIntent.description)
      
      case 'execute':
        return await this.handleWorkflowExecution(workflowIntent.workflowId, workflowIntent.params)
      
      case 'list':
        return await this.handleWorkflowList()
      
      case 'modify':
        return await this.handleWorkflowModification(workflowIntent.workflowId, workflowIntent.changes)
      
      default:
        return await this.handleGenericWorkflow(userInput)
    }
  }
  
  private async handleWorkflowCreation(description: string): Promise<string> {
    try {
      // ç”Ÿæˆå·¥ä½œæµ
      const workflow = await this.workflowGenerator.generateFromDescription(description)
      
      // ä¿å­˜å·¥ä½œæµ
      const savedWorkflow = await this.workflowEngine.saveWorkflow(workflow)
      
      return `âœ… å·¥ä½œæµåˆ›å»ºæˆåŠŸï¼

**å·¥ä½œæµåç§°ï¼š** ${workflow.name}
**æè¿°ï¼š** ${workflow.description}
**æ­¥éª¤æ•°ï¼š** ${workflow.steps.length}

å·¥ä½œæµå·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥è¯´"æ‰§è¡Œå·¥ä½œæµ ${workflow.name}"æ¥è¿è¡Œå®ƒï¼Œæˆ–è€…è¯´"ä¿®æ”¹å·¥ä½œæµ"æ¥è°ƒæ•´é…ç½®ã€‚

éœ€è¦ç«‹å³æ‰§è¡Œè¿™ä¸ªå·¥ä½œæµå—ï¼Ÿ`
      
    } catch (error) {
      return `âŒ å·¥ä½œæµåˆ›å»ºå¤±è´¥ï¼š${error.message}\n\nè¯·å°è¯•æ›´è¯¦ç»†åœ°æè¿°æ‚¨æƒ³è¦çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ã€‚`
    }
  }
  
  private async handleWorkflowExecution(workflowId: string, params?: any): Promise<string> {
    try {
      const result = await this.workflowEngine.executeWorkflow(workflowId, params)
      
      if (result.status === 'completed') {
        return `âœ… å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼\n\næ‰§è¡Œæ—¶é—´ï¼š${result.executionTime}ms\nç»“æœï¼š${JSON.stringify(result.result, null, 2)}`
      } else {
        return `âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼š${result.error}`
      }
      
    } catch (error) {
      return `âŒ å·¥ä½œæµæ‰§è¡Œå‡ºé”™ï¼š${error.message}`
    }
  }
}
```

è¿™æ ·çš„è®¾è®¡å®ç°äº†çœŸæ­£çš„"å¯¹è¯å³ç•Œé¢"ä½“éªŒï¼š

1. **æç®€ç•Œé¢**ï¼šåªæœ‰ä¸€ä¸ªå¯¹è¯æ¡†ï¼Œæ²¡æœ‰ä»»ä½•ä¼ ç»ŸUIå…ƒç´ 
2. **æ™ºèƒ½æ„å›¾è¯†åˆ«**ï¼šé€šè¿‡LLMç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤
3. **å¯¹è¯å¼é…ç½®**ï¼šæ‰€æœ‰ç³»ç»Ÿè®¾ç½®éƒ½é€šè¿‡å¯¹è¯å®Œæˆ
4. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥**ï¼šæ ¹æ®å½“å‰çŠ¶æ€æä¾›ç›¸åº”çš„æç¤ºå’Œå»ºè®®
5. **å¤šæ¨¡æ€äº¤äº’**ï¼šæ”¯æŒæ–‡æœ¬ã€æ–‡ä»¶æ‹–æ‹½ã€è¯­éŸ³è¾“å…¥

ç”¨æˆ·å¯ä»¥è¯´ï¼š
- "åˆ‡æ¢åˆ°æ·±è‰²ä¸»é¢˜"
- "è®¾ç½®OpenAI APIå¯†é’¥"
- "ä¸Šä¼ è¿™äº›PDFæ–‡æ¡£åˆ°çŸ¥è¯†åº“"
- "åˆ›å»ºä¸€ä¸ªæ¯å¤©å‘é€é‚®ä»¶çš„å·¥ä½œæµ"
- "æ˜¾ç¤ºå½“å‰ç³»ç»Ÿè®¾ç½®"

æ‰€æœ‰æ“ä½œéƒ½é€šè¿‡è‡ªç„¶å¯¹è¯å®Œæˆï¼Œæ— éœ€ä»»ä½•ä¼ ç»Ÿç•Œé¢å…ƒç´ ã€‚### 7.
 å®‰å…¨é…ç½®é¡µé¢ç³»ç»Ÿ

#### é…ç½®é¡µé¢ç®¡ç†å™¨
```typescript
// src/services/config/SecureConfigManager.ts
export class SecureConfigManager {
  private configWindows: Map<string, BrowserWindow> = new Map()
  
  async openSecureConfigPage(configType: string, params?: any): Promise<void> {
    // æ£€æŸ¥æ˜¯å¦å·²æœ‰é…ç½®çª—å£æ‰“å¼€
    const existingWindow = this.configWindows.get(configType)
    if (existingWindow && !existingWindow.isDestroyed()) {
      existingWindow.focus()
      return
    }
    
    // åˆ›å»ºå®‰å…¨çš„é…ç½®çª—å£
    const configWindow = new BrowserWindow({
      width: 600,
      height: 400,
      modal: true,
      resizable: false,
      webPreferences: {
        nodeIntegration: false,
        contextIsolation: true,
        preload: path.join(__dirname, 'config-preload.js')
      },
      titleBarStyle: 'hiddenInset', // macOSé£æ ¼
      vibrancy: 'under-window' // æ¯›ç»ç’ƒæ•ˆæœ
    })
    
    // åŠ è½½å¯¹åº”çš„é…ç½®é¡µé¢
    await configWindow.loadFile(`config-pages/${configType}.html`, {
      query: params ? new URLSearchParams(params).toString() : undefined
    })
    
    // å­˜å‚¨çª—å£å¼•ç”¨
    this.configWindows.set(configType, configWindow)
    
    // çª—å£å…³é—­æ—¶æ¸…ç†å¼•ç”¨
    configWindow.on('closed', () => {
      this.configWindows.delete(configType)
    })
  }
  
  async handleConfigComplete(configType: string, data: any): Promise<void> {
    // å®‰å…¨å¤„ç†é…ç½®æ•°æ®
    switch (configType) {
      case 'api-keys':
        await this.handleApiKeyConfig(data)
        break
      case 'llm-settings':
        await this.handleLLMConfig(data)
        break
      case 'system-settings':
        await this.handleSystemConfig(data)
        break
    }
    
    // å…³é—­é…ç½®çª—å£
    const window = this.configWindows.get(configType)
    if (window && !window.isDestroyed()) {
      window.close()
    }
    
    // é€šçŸ¥ä¸»çª—å£é…ç½®å®Œæˆ
    this.notifyMainWindow(configType, 'completed')
  }
  
  private async handleApiKeyConfig(data: any): Promise<void> {
    // åŠ å¯†å­˜å‚¨APIå¯†é’¥
    const encryptedKey = await this.encryptSensitiveData(data.apiKey)
    
    await this.configStore.setSecureConfig(`api_key_${data.provider}`, encryptedKey)
    
    // éªŒè¯APIå¯†é’¥æœ‰æ•ˆæ€§
    const isValid = await this.validateApiKey(data.provider, data.apiKey)
    if (!isValid) {
      throw new Error('APIå¯†é’¥éªŒè¯å¤±è´¥')
    }
  }
}
```

#### å¯¹è¯å¼é…ç½®è°ƒåº¦
```typescript
// æ›´æ–°å¯¹è¯å¼é…ç½®ç®¡ç†å™¨
export class DialogueConfigManager {
  private secureConfigManager: SecureConfigManager
  
  async handleSystemConfig(userInput: string, params: any): Promise<string> {
    const configIntent = await this.parseConfigIntent(userInput)
    
    switch (configIntent.action) {
      case 'open_bor_config':
        await this.secureConfigManager.openSecureConfigPage('system-settings')
        return 'æ­£åœ¨æ‰“å¼€Borç³»ç»Ÿé…ç½®é¡µé¢...'
      
      case 'open_llm_config':
        await this.secureConfigManager.openSecureConfigPage('llm-settings')
        return 'æ­£åœ¨æ‰“å¼€LLMé…ç½®é¡µé¢ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­å®‰å…¨åœ°ç®¡ç†APIå¯†é’¥å’Œæ¨¡å‹è®¾ç½®...'
      
      case 'theme_change':
        return await this.handleThemeChange(configIntent.value)
      
      case 'model_switch':
        return await this.handleModelSwitch(configIntent.model)
      
      case 'settings_view':
        return await this.handleSettingsView()
      
      default:
        return await this.handleGenericConfig(userInput)
    }
  }
  
  private async parseConfigIntent(userInput: string): Promise<ConfigIntent> {
    // è¯†åˆ«é…ç½®ç›¸å…³çš„å…³é”®è¯
    const configKeywords = {
      'bor_config': ['é…ç½®bor', 'ç³»ç»Ÿé…ç½®', 'è®¾ç½®bor', 'borè®¾ç½®'],
      'llm_config': ['é…ç½®llm', 'llmé…ç½®', 'æ¨¡å‹é…ç½®', 'é…ç½®æ¨¡å‹', 'apié…ç½®'],
      'theme_change': ['åˆ‡æ¢ä¸»é¢˜', 'æ·±è‰²æ¨¡å¼', 'æµ…è‰²æ¨¡å¼', 'ä¸»é¢˜'],
      'model_switch': ['åˆ‡æ¢æ¨¡å‹', 'ä½¿ç”¨æ¨¡å‹', 'æ¢æ¨¡å‹'],
      'settings_view': ['æŸ¥çœ‹è®¾ç½®', 'å½“å‰é…ç½®', 'ç³»ç»ŸçŠ¶æ€']
    }
    
    for (const [action, keywords] of Object.entries(configKeywords)) {
      if (keywords.some(keyword => userInput.includes(keyword))) {
        return { action, confidence: 0.9 }
      }
    }
    
    return { action: 'unknown', confidence: 0.1 }
  }
}
```

#### é…ç½®é¡µé¢ç¤ºä¾‹
```html
<!-- config-pages/llm-settings.html -->
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>LLMé…ç½®</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', sans-serif;
            background: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(20px);
            margin: 0;
            padding: 20px;
        }
        
        .config-section {
            background: rgba(255, 255, 255, 0.6);
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 16px;
            border: 1px solid rgba(255, 255, 255, 0.2);
        }
        
        .input-group {
            margin-bottom: 16px;
        }
        
        label {
            display: block;
            margin-bottom: 8px;
            font-weight: 500;
            color: #1d1d1f;
        }
        
        input, select {
            width: 100%;
            padding: 12px;
            border: 1px solid #d1d1d6;
            border-radius: 8px;
            font-size: 16px;
            background: rgba(255, 255, 255, 0.8);
        }
        
        input[type="password"] {
            font-family: monospace;
        }
        
        .button-group {
            display: flex;
            gap: 12px;
            justify-content: flex-end;
            margin-top: 24px;
        }
        
        button {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.2s ease;
        }
        
        .primary-button {
            background: #007AFF;
            color: white;
        }
        
        .secondary-button {
            background: rgba(142, 142, 147, 0.12);
            color: #1d1d1f;
        }
    </style>
</head>
<body>
    <div class="config-section">
        <h2>LLMæ¨¡å‹é…ç½®</h2>
        
        <div class="input-group">
            <label for="provider">æ¨¡å‹æä¾›å•†</label>
            <select id="provider" onchange="updateProviderFields()">
                <option value="ollama">Ollama (æœ¬åœ°)</option>
                <option value="openrouter">OpenRouter</option>
                <option value="openai">OpenAI</option>
                <option value="openai-compatible">OpenAIå…¼å®¹ (è‡ªå®šä¹‰)</option>
                <option value="zhipu">æ™ºè°±AI</option>
                <option value="gemini">Google Gemini</option>
            </select>
        </div>
        
        <div class="input-group">
            <label for="apiKey">APIå¯†é’¥</label>
            <input type="password" id="apiKey" placeholder="è¾“å…¥æ‚¨çš„APIå¯†é’¥">
        </div>
        
        <div class="input-group">
            <label for="baseUrl">APIåŸºç¡€URL (å¯é€‰)</label>
            <input type="url" id="baseUrl" placeholder="https://api.openai.com/v1">
        </div>
        
        <div class="input-group">
            <label for="defaultModel">é»˜è®¤æ¨¡å‹</label>
            <select id="defaultModel">
                <!-- åŠ¨æ€åŠ è½½ï¼Œæ ¹æ®é€‰æ‹©çš„ä¾›åº”å•†æ›´æ–° -->
            </select>
        </div>
        
        <div class="input-group" id="customNameGroup" style="display: none;">
            <label for="customName">è‡ªå®šä¹‰åç§°</label>
            <input type="text" id="customName" placeholder="ä¸ºè¿™ä¸ªé…ç½®èµ·ä¸ªåå­—">
        </div>
    </div>
    
    <div class="button-group">
        <button class="secondary-button" onclick="window.close()">å–æ¶ˆ</button>
        <button class="primary-button" onclick="saveConfig()">ä¿å­˜é…ç½®</button>
    </div>
    
    <script>
        async function saveConfig() {
            const config = {
                provider: document.getElementById('provider').value,
                apiKey: document.getElementById('apiKey').value,
                baseUrl: document.getElementById('baseUrl').value,
                defaultModel: document.getElementById('defaultModel').value
            }
            
            // é€šè¿‡å®‰å…¨çš„IPCé€šé“å‘é€é…ç½®
            await window.electronAPI.saveSecureConfig('llm-settings', config)
        }
    </script>
</body>
</html>
```

è¿™æ ·çš„è®¾è®¡å®ç°äº†å®‰å…¨ä¸ä¾¿åˆ©çš„å¹³è¡¡ï¼š

1. **å¯¹è¯è§¦å‘**ï¼šç”¨æˆ·è¯´"é…ç½®LLM"æˆ–"é…ç½®Bor"æ¥æ‰“å¼€ç›¸åº”é…ç½®é¡µé¢
2. **å®‰å…¨è¾“å…¥**ï¼šæ•æ„Ÿä¿¡æ¯åœ¨ä¸“é—¨çš„é…ç½®çª—å£ä¸­å¤„ç†ï¼Œä¸ä¼šå‡ºç°åœ¨å¯¹è¯è®°å½•ä¸­
3. **Appleé£æ ¼**ï¼šé…ç½®é¡µé¢é‡‡ç”¨æ¯›ç»ç’ƒæ•ˆæœå’Œç³»ç»Ÿå­—ä½“ï¼Œä¿æŒä¸€è‡´çš„è®¾è®¡é£æ ¼
4. **åŠ å¯†å­˜å‚¨**ï¼šAPIå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯ç»è¿‡åŠ å¯†åå­˜å‚¨
5. **å³æ—¶éªŒè¯**ï¼šé…ç½®ä¿å­˜æ—¶ç«‹å³éªŒè¯APIå¯†é’¥çš„æœ‰æ•ˆæ€§

ç”¨æˆ·ä½“éªŒæµç¨‹ï¼š
1. åœ¨å¯¹è¯ä¸­è¯´"é…ç½®LLM"
2. ç³»ç»Ÿæ‰“å¼€å®‰å…¨çš„é…ç½®çª—å£
3. ç”¨æˆ·åœ¨ä¸“é—¨é¡µé¢ä¸­è¾“å…¥APIå¯†é’¥
4. é…ç½®å®Œæˆåè¿”å›å¯¹è¯ç•Œé¢
5. ç³»ç»Ÿç¡®è®¤é…ç½®æˆåŠŸ####
 åŸºäºæˆç†Ÿæ–¹æ¡ˆçš„ä¾›åº”å•†é…ç½®
```typescript
// src/services/llm/ProviderConfigManager.ts
// åŸºäº LLMChat å’Œ Chatbot UI çš„æˆç†Ÿé…ç½®æ–¹æ¡ˆ

export class ProviderConfigManager {
  private static readonly PROVIDER_CONFIGS = {
    ollama: {
      name: 'Ollama',
      description: 'æœ¬åœ°è¿è¡Œçš„å¼€æºæ¨¡å‹',
      requiresApiKey: false,
      defaultBaseURL: 'http://localhost:11434',
      supportedModels: [], // åŠ¨æ€è·å–
      configFields: ['baseURL'],
      testEndpoint: '/api/tags'
    },
    
    openrouter: {
      name: 'OpenRouter',
      description: 'ç»Ÿä¸€çš„AIæ¨¡å‹APIç½‘å…³',
      requiresApiKey: true,
      defaultBaseURL: 'https://openrouter.ai/api/v1',
      supportedModels: [], // åŠ¨æ€è·å–
      configFields: ['apiKey'],
      testEndpoint: '/models'
    },
    
    openai: {
      name: 'OpenAI',
      description: 'OpenAIå®˜æ–¹API',
      requiresApiKey: true,
      defaultBaseURL: 'https://api.openai.com/v1',
      supportedModels: ['gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo'],
      configFields: ['apiKey'],
      testEndpoint: '/models'
    },
    
    'openai-compatible': {
      name: 'OpenAIå…¼å®¹',
      description: 'å…¼å®¹OpenAI APIçš„è‡ªå®šä¹‰æœåŠ¡',
      requiresApiKey: true,
      defaultBaseURL: '',
      supportedModels: [], // ç”¨æˆ·è‡ªå®šä¹‰
      configFields: ['apiKey', 'baseURL', 'customName'],
      testEndpoint: '/models'
    },
    
    zhipu: {
      name: 'æ™ºè°±AI',
      description: 'æ™ºè°±AIå¤§æ¨¡å‹æœåŠ¡',
      requiresApiKey: true,
      defaultBaseURL: 'https://open.bigmodel.cn/api/paas/v4',
      supportedModels: ['glm-4', 'glm-4v', 'glm-3-turbo'],
      configFields: ['apiKey'],
      testEndpoint: '/models'
    },
    
    gemini: {
      name: 'Google Gemini',
      description: 'Googleçš„Geminiæ¨¡å‹',
      requiresApiKey: true,
      defaultBaseURL: 'https://generativelanguage.googleapis.com/v1beta',
      supportedModels: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro-vision'],
      configFields: ['apiKey'],
      testEndpoint: '/models'
    }
  }
  
  static getProviderConfig(providerType: string): ProviderConfigSchema {
    return this.PROVIDER_CONFIGS[providerType] || null
  }
  
  static getAllProviders(): ProviderConfigSchema[] {
    return Object.entries(this.PROVIDER_CONFIGS).map(([type, config]) => ({
      type,
      ...config
    }))
  }
  
  async validateProviderConfig(providerType: string, config: any): Promise<ValidationResult> {
    const providerSchema = ProviderConfigManager.getProviderConfig(providerType)
    
    if (!providerSchema) {
      return { valid: false, error: 'ä¸æ”¯æŒçš„ä¾›åº”å•†ç±»å‹' }
    }
    
    // æ£€æŸ¥å¿…éœ€å­—æ®µ
    for (const field of providerSchema.configFields) {
      if (field === 'apiKey' && providerSchema.requiresApiKey && !config[field]) {
        return { valid: false, error: 'APIå¯†é’¥æ˜¯å¿…éœ€çš„' }
      }
      if (field === 'baseURL' && !config[field] && !providerSchema.defaultBaseURL) {
        return { valid: false, error: 'åŸºç¡€URLæ˜¯å¿…éœ€çš„' }
      }
    }
    
    // æµ‹è¯•è¿æ¥
    try {
      await this.testProviderConnection(providerType, config)
      return { valid: true }
    } catch (error) {
      return { valid: false, error: `è¿æ¥æµ‹è¯•å¤±è´¥: ${error.message}` }
    }
  }
  
  private async testProviderConnection(providerType: string, config: any): Promise<void> {
    const providerSchema = ProviderConfigManager.getProviderConfig(providerType)
    const baseURL = config.baseURL || providerSchema.defaultBaseURL
    const testURL = `${baseURL}${providerSchema.testEndpoint}`
    
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    }
    
    if (config.apiKey) {
      if (providerType === 'openrouter') {
        headers['Authorization'] = `Bearer ${config.apiKey}`
        headers['HTTP-Referer'] = 'https://bor-ai.app'
      } else if (providerType === 'gemini') {
        // Geminiä½¿ç”¨æŸ¥è¯¢å‚æ•°
      } else {
        headers['Authorization'] = `Bearer ${config.apiKey}`
      }
    }
    
    const response = await fetch(testURL, { headers })
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`)
    }
  }
}
```

#### åŠ¨æ€é…ç½®é¡µé¢è„šæœ¬
```javascript
// config-pages/llm-settings.js
const PROVIDER_CONFIGS = {
  ollama: {
    models: [], // åŠ¨æ€è·å–
    fields: ['baseURL']
  },
  openrouter: {
    models: [], // åŠ¨æ€è·å–
    fields: ['apiKey']
  },
  openai: {
    models: ['gpt-4', 'gpt-4-turbo', 'gpt-3.5-turbo'],
    fields: ['apiKey']
  },
  'openai-compatible': {
    models: [], // ç”¨æˆ·è‡ªå®šä¹‰
    fields: ['apiKey', 'baseURL', 'customName']
  },
  zhipu: {
    models: ['glm-4', 'glm-4v', 'glm-3-turbo'],
    fields: ['apiKey']
  },
  gemini: {
    models: ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-pro-vision'],
    fields: ['apiKey']
  }
}

function updateProviderFields() {
  const provider = document.getElementById('provider').value
  const config = PROVIDER_CONFIGS[provider]
  
  // æ˜¾ç¤º/éšè—ç›¸å…³å­—æ®µ
  document.getElementById('apiKeyGroup').style.display = 
    config.fields.includes('apiKey') ? 'block' : 'none'
  
  document.getElementById('baseUrlGroup').style.display = 
    config.fields.includes('baseURL') ? 'block' : 'none'
  
  document.getElementById('customNameGroup').style.display = 
    config.fields.includes('customName') ? 'block' : 'none'
  
  // æ›´æ–°æ¨¡å‹åˆ—è¡¨
  updateModelList(provider)
  
  // è®¾ç½®é»˜è®¤å€¼
  if (provider === 'ollama') {
    document.getElementById('baseUrl').value = 'http://localhost:11434'
  } else if (provider === 'openrouter') {
    document.getElementById('baseUrl').value = 'https://openrouter.ai/api/v1'
  }
}

async function updateModelList(provider) {
  const modelSelect = document.getElementById('defaultModel')
  modelSelect.innerHTML = '<option value="">åŠ è½½ä¸­...</option>'
  
  try {
    if (provider === 'ollama') {
      // åŠ¨æ€è·å–Ollamaæ¨¡å‹
      const models = await window.electronAPI.getOllamaModels()
      updateSelectOptions(modelSelect, models)
    } else if (provider === 'openrouter') {
      // åŠ¨æ€è·å–OpenRouteræ¨¡å‹
      const models = await window.electronAPI.getOpenRouterModels()
      updateSelectOptions(modelSelect, models.slice(0, 20)) // é™åˆ¶æ˜¾ç¤ºæ•°é‡
    } else {
      // ä½¿ç”¨é¢„å®šä¹‰æ¨¡å‹åˆ—è¡¨
      const models = PROVIDER_CONFIGS[provider].models
      updateSelectOptions(modelSelect, models)
    }
  } catch (error) {
    modelSelect.innerHTML = '<option value="">è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥</option>'
  }
}

function updateSelectOptions(select, options) {
  select.innerHTML = ''
  options.forEach(option => {
    const optionElement = document.createElement('option')
    optionElement.value = option
    optionElement.textContent = option
    select.appendChild(optionElement)
  })
}

async function saveConfig() {
  const provider = document.getElementById('provider').value
  const config = {
    type: provider,
    apiKey: document.getElementById('apiKey').value,
    baseURL: document.getElementById('baseUrl').value,
    defaultModel: document.getElementById('defaultModel').value,
    customName: document.getElementById('customName').value
  }
  
  // æ˜¾ç¤ºä¿å­˜çŠ¶æ€
  const saveButton = document.querySelector('.primary-button')
  const originalText = saveButton.textContent
  saveButton.textContent = 'ä¿å­˜ä¸­...'
  saveButton.disabled = true
  
  try {
    // éªŒè¯é…ç½®
    const validation = await window.electronAPI.validateProviderConfig(provider, config)
    
    if (!validation.valid) {
      alert(`é…ç½®éªŒè¯å¤±è´¥: ${validation.error}`)
      return
    }
    
    // ä¿å­˜é…ç½®
    await window.electronAPI.saveSecureConfig('llm-settings', config)
    
    // æ˜¾ç¤ºæˆåŠŸæ¶ˆæ¯
    saveButton.textContent = 'ä¿å­˜æˆåŠŸ!'
    setTimeout(() => {
      window.close()
    }, 1000)
    
  } catch (error) {
    alert(`ä¿å­˜å¤±è´¥: ${error.message}`)
    saveButton.textContent = originalText
    saveButton.disabled = false
  }
}

// é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
document.addEventListener('DOMContentLoaded', () => {
  updateProviderFields()
})
```

è¿™æ ·çš„è®¾è®¡å……åˆ†åˆ©ç”¨äº† LLMChat å’Œ Chatbot UI çš„æˆç†Ÿæ–¹æ¡ˆï¼š

1. **å¤šä¾›åº”å•†æ”¯æŒ**ï¼šè¦†ç›–ä½ å¸¸ç”¨çš„ Ollamaã€OpenRouterã€OpenAIå…¼å®¹ã€æ™ºè°±ã€Gemini
2. **åŠ¨æ€é…ç½®**ï¼šæ ¹æ®é€‰æ‹©çš„ä¾›åº”å•†åŠ¨æ€æ˜¾ç¤ºç›¸å…³é…ç½®å­—æ®µ
3. **æ¨¡å‹è‡ªåŠ¨å‘ç°**ï¼šå¯¹äº Ollama å’Œ OpenRouterï¼Œè‡ªåŠ¨è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
4. **è¿æ¥æµ‹è¯•**ï¼šä¿å­˜å‰è‡ªåŠ¨æµ‹è¯•è¿æ¥æœ‰æ•ˆæ€§
5. **ç”¨æˆ·å‹å¥½**ï¼šæ¸…æ™°çš„ç•Œé¢å’Œé”™è¯¯æç¤º

è¿™æ ·æ—¢ä¿æŒäº†é…ç½®çš„çµæ´»æ€§ï¼Œåˆæä¾›äº†è‰¯å¥½çš„ç”¨æˆ·ä½“éªŒã€‚##
# 8. è‡ªå‡çº§å’Œä»£ç ç”Ÿæˆç³»ç»Ÿ

#### MCP é›†æˆå¼€å‘ç¯å¢ƒå¯¹æ¥
```typescript
// src/services/development/IDEIntegration.ts
export class IDEIntegration {
  private mcpManager: MCPManager
  
  constructor(mcpManager: MCPManager) {
    this.mcpManager = mcpManager
  }
  
  async setupKiroIntegration(): Promise<void> {
    // æ³¨å†Œ Kiro MCP æœåŠ¡å™¨
    await this.mcpManager.registerServer({
      name: 'kiro-ide',
      transport: {
        type: 'stdio',
        command: 'kiro',
        args: ['mcp-server']
      },
      autoStart: true,
      capabilities: [
        'code-generation',
        'file-operations', 
        'project-analysis',
        'refactoring'
      ]
    })
  }
  
  async setupVSCodeIntegration(): Promise<void> {
    // é€šè¿‡ VS Code MCP æ‰©å±•å¯¹æ¥
    await this.mcpManager.registerServer({
      name: 'vscode-integration',
      transport: {
        type: 'websocket',
        url: 'ws://localhost:3001/vscode-mcp'
      },
      autoStart: false,
      capabilities: [
        'workspace-operations',
        'extension-management',
        'debugging',
        'git-operations'
      ]
    })
  }
}
```

#### è‡ªå‡çº§å·¥ä½œæµç³»ç»Ÿ
```typescript
// src/services/upgrade/SelfUpgradeManager.ts
export class SelfUpgradeManager {
  private workflowEngine: WorkflowEngine
  private ideIntegration: IDEIntegration
  
  async generateUpgradeWorkflow(featureDescription: string): Promise<WorkflowDefinition> {
    const upgradePrompt = `
ä½œä¸º Bor æ™ºèƒ½ä½“ä¸­æ¢çš„è‡ªå‡çº§ç³»ç»Ÿï¼Œè¯·ä¸ºä»¥ä¸‹åŠŸèƒ½éœ€æ±‚ç”Ÿæˆä¸€ä¸ªè‡ªåŠ¨åŒ–å‡çº§å·¥ä½œæµï¼š

åŠŸèƒ½æè¿°ï¼š${featureDescription}

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«ä»¥ä¸‹æ­¥éª¤çš„å·¥ä½œæµï¼š
1. éœ€æ±‚åˆ†æå’ŒæŠ€æœ¯æ–¹æ¡ˆè®¾è®¡
2. é€šè¿‡ Kiro/VS Code MCP ç”Ÿæˆä»£ç 
3. ä»£ç é›†æˆå’Œæµ‹è¯•
4. è‡ªåŠ¨éƒ¨ç½²å’ŒéªŒè¯
5. å›æ»šæœºåˆ¶ï¼ˆå¦‚æœå‡ºç°é—®é¢˜ï¼‰

å·¥ä½œæµåº”è¯¥èƒ½å¤Ÿï¼š
- è‡ªåŠ¨åˆ†æç°æœ‰ä»£ç ç»“æ„
- ç”Ÿæˆç¬¦åˆé¡¹ç›®æ¶æ„çš„æ–°ä»£ç 
- è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
- å®‰å…¨åœ°é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿä¸­
`
    
    return await this.workflowEngine.generateWorkflow(upgradePrompt)
  }
  
  async executeSelfUpgrade(featureDescription: string): Promise<UpgradeResult> {
    try {
      // 1. ç”Ÿæˆå‡çº§å·¥ä½œæµ
      const workflow = await this.generateUpgradeWorkflow(featureDescription)
      
      // 2. åˆ›å»ºå‡çº§åˆ†æ”¯
      await this.createUpgradeBranch(workflow.id)
      
      // 3. æ‰§è¡Œå‡çº§å·¥ä½œæµ
      const result = await this.workflowEngine.executeWorkflow(workflow, {
        targetFeature: featureDescription,
        safetyMode: true,
        autoRollback: true
      })
      
      // 4. éªŒè¯å‡çº§ç»“æœ
      const validation = await this.validateUpgrade(result)
      
      if (validation.success) {
        await this.commitUpgrade(workflow.id)
        return { status: 'success', changes: validation.changes }
      } else {
        await this.rollbackUpgrade(workflow.id)
        return { status: 'failed', error: validation.error }
      }
      
    } catch (error) {
      return { status: 'error', error: error.message }
    }
  }
}
```

#### å¯¹è¯å¼è‡ªå‡çº§æ¥å£
```typescript
// src/services/upgrade/DialogueUpgradeManager.ts
export class DialogueUpgradeManager {
  private selfUpgradeManager: SelfUpgradeManager
  
  async handleUpgradeRequest(userInput: string): Promise<string> {
    const upgradeIntent = await this.parseUpgradeIntent(userInput)
    
    switch (upgradeIntent.type) {
      case 'add_feature':
        return await this.handleFeatureAddition(upgradeIntent.description)
      
      case 'fix_bug':
        return await this.handleBugFix(upgradeIntent.description)
      
      case 'optimize_performance':
        return await this.handlePerformanceOptimization(upgradeIntent.target)
      
      case 'update_ui':
        return await this.handleUIUpdate(upgradeIntent.changes)
      
      default:
        return await this.handleGenericUpgrade(userInput)
    }
  }
  
  private async handleFeatureAddition(description: string): Promise<string> {
    return `ğŸš€ å¼€å§‹ä¸ºæ‚¨æ·»åŠ æ–°åŠŸèƒ½...

**åŠŸèƒ½æè¿°ï¼š** ${description}

æˆ‘å°†é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥å®ç°ï¼š
1. ğŸ“‹ åˆ†æåŠŸèƒ½éœ€æ±‚å’ŒæŠ€æœ¯æ–¹æ¡ˆ
2. ğŸ”§ é€šè¿‡ Kiro MCP ç”Ÿæˆç›¸å…³ä»£ç 
3. ğŸ§ª è‡ªåŠ¨è¿è¡Œæµ‹è¯•ç¡®ä¿è´¨é‡
4. ğŸ”„ å®‰å…¨é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿä¸­

è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œæˆ‘ä¼šå®æ—¶å‘æ‚¨æ±‡æŠ¥è¿›å±•ã€‚

æ˜¯å¦ç»§ç»­æ‰§è¡Œè‡ªåŠ¨å‡çº§ï¼Ÿ`
  }
  
  private async executeAutoUpgrade(description: string): Promise<string> {
    const result = await this.selfUpgradeManager.executeSelfUpgrade(description)
    
    switch (result.status) {
      case 'success':
        return `âœ… è‡ªå‡çº§æˆåŠŸå®Œæˆï¼

**æ–°å¢åŠŸèƒ½ï¼š** ${description}
**ä¿®æ”¹æ–‡ä»¶ï¼š** ${result.changes.modifiedFiles.length} ä¸ª
**æ–°å¢æ–‡ä»¶ï¼š** ${result.changes.newFiles.length} ä¸ª
**æµ‹è¯•é€šè¿‡ï¼š** ${result.changes.testsPass} ä¸ª

æ–°åŠŸèƒ½å·²ç»é›†æˆåˆ°ç³»ç»Ÿä¸­ï¼Œæ‚¨ç°åœ¨å°±å¯ä»¥ä½¿ç”¨äº†ï¼`

      case 'failed':
        return `âš ï¸ è‡ªå‡çº§å¤±è´¥ï¼Œå·²è‡ªåŠ¨å›æ»š

**å¤±è´¥åŸå› ï¼š** ${result.error}

ç³»ç»Ÿå·²æ¢å¤åˆ°å‡çº§å‰çš„çŠ¶æ€ï¼Œæ²¡æœ‰ä»»ä½•å½±å“ã€‚æ‚¨å¯ä»¥ï¼š
1. é‡æ–°æè¿°åŠŸèƒ½éœ€æ±‚ï¼Œæˆ‘ä¼šå°è¯•ä¸åŒçš„å®ç°æ–¹æ¡ˆ
2. æ‰‹åŠ¨æ£€æŸ¥ç›¸å…³é…ç½®
3. ç¨åå†è¯•`

      case 'error':
        return `âŒ å‡çº§è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š** ${result.error}

ä¸ºäº†ç³»ç»Ÿå®‰å…¨ï¼Œå‡çº§å·²è¢«ä¸­æ­¢ã€‚å»ºè®®æ£€æŸ¥ç³»ç»ŸçŠ¶æ€æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚`
    }
  }
}
```

#### MCP å·¥å…·å®šä¹‰
```yaml
# mcp-servers/bor-self-upgrade.yaml
name: "bor-self-upgrade"
description: "Boræ™ºèƒ½ä½“ä¸­æ¢è‡ªå‡çº§MCPæœåŠ¡å™¨"

tools:
  - name: "analyze_codebase"
    description: "åˆ†æç°æœ‰ä»£ç ç»“æ„å’Œæ¶æ„"
    parameters:
      type: "object"
      properties:
        target_feature:
          type: "string"
          description: "è¦åˆ†æçš„åŠŸèƒ½é¢†åŸŸ"
        
  - name: "generate_code"
    description: "åŸºäºéœ€æ±‚ç”Ÿæˆä»£ç "
    parameters:
      type: "object"
      properties:
        feature_spec:
          type: "string"
          description: "åŠŸèƒ½è§„æ ¼è¯´æ˜"
        code_style:
          type: "string"
          description: "ä»£ç é£æ ¼è¦æ±‚"
          
  - name: "run_tests"
    description: "æ‰§è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•"
    parameters:
      type: "object"
      properties:
        test_scope:
          type: "string"
          enum: ["unit", "integration", "e2e", "all"]
          
  - name: "deploy_changes"
    description: "éƒ¨ç½²ä»£ç å˜æ›´"
    parameters:
      type: "object"
      properties:
        deployment_mode:
          type: "string"
          enum: ["safe", "fast", "rollback"]
```

#### ç¤ºä¾‹å¯¹è¯æµç¨‹
```
ç”¨æˆ·ï¼šæˆ‘å¸Œæœ›æ·»åŠ ä¸€ä¸ªè¯­éŸ³è¾“å…¥åŠŸèƒ½

Borï¼šğŸš€ å¼€å§‹ä¸ºæ‚¨æ·»åŠ è¯­éŸ³è¾“å…¥åŠŸèƒ½...

æˆ‘å°†é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
1. ğŸ“‹ åˆ†æç°æœ‰è¾“å…¥ç»„ä»¶æ¶æ„
2. ğŸ”§ é€šè¿‡ Kiro MCP ç”Ÿæˆè¯­éŸ³è¯†åˆ«ä»£ç 
3. ğŸ¨ æ›´æ–° UI ç»„ä»¶æ·»åŠ è¯­éŸ³æŒ‰é’®
4. ğŸ§ª è‡ªåŠ¨æµ‹è¯•è¯­éŸ³åŠŸèƒ½
5. ğŸ”„ é›†æˆåˆ°èŠå¤©ç•Œé¢ä¸­

é¢„è®¡éœ€è¦ 3-5 åˆ†é’Ÿï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ

ç”¨æˆ·ï¼šç»§ç»­

Borï¼šâœ… æ­£åœ¨åˆ†æä»£ç ç»“æ„...
ğŸ“ å·²è¯†åˆ«è¾“å…¥ç»„ä»¶ï¼šsrc/components/ChatInput.vue
ğŸ” æ£€æµ‹åˆ°ç°æœ‰å¤šæ¨¡æ€æ”¯æŒæ¶æ„

ğŸ”§ æ­£åœ¨ç”Ÿæˆè¯­éŸ³è¯†åˆ«ä»£ç ...
ğŸ“ å·²ç”Ÿæˆï¼šsrc/services/speech/SpeechRecognition.ts
ğŸ¨ å·²æ›´æ–°ï¼šChatInput.vue æ·»åŠ è¯­éŸ³æŒ‰é’®

ğŸ§ª æ­£åœ¨è¿è¡Œæµ‹è¯•...
âœ… å•å…ƒæµ‹è¯•é€šè¿‡ï¼š15/15
âœ… é›†æˆæµ‹è¯•é€šè¿‡ï¼š8/8

ğŸ”„ æ­£åœ¨é›†æˆæ–°åŠŸèƒ½...
âœ… è¯­éŸ³è¾“å…¥åŠŸèƒ½æ·»åŠ æˆåŠŸï¼

ç°åœ¨æ‚¨å¯ä»¥ç‚¹å‡»èŠå¤©æ¡†æ—è¾¹çš„éº¦å…‹é£å›¾æ ‡ä½¿ç”¨è¯­éŸ³è¾“å…¥äº†ï¼
```

è¿™ä¸ªè®¾è®¡å®ç°äº†çœŸæ­£çš„"è‡ªæˆ‘è¿›åŒ–"èƒ½åŠ›ï¼š

1. **å¯¹è¯å¼å‡çº§**ï¼šç”¨æˆ·åªéœ€æè¿°æƒ³è¦çš„åŠŸèƒ½ï¼Œç³»ç»Ÿè‡ªåŠ¨å®ç°
2. **MCP é›†æˆå¼€å‘**ï¼šé€šè¿‡ Kiro æˆ– VS Code çš„ MCP æ¥å£ç”Ÿæˆé«˜è´¨é‡ä»£ç 
3. **å®‰å…¨å‡çº§æœºåˆ¶**ï¼šè‡ªåŠ¨æµ‹è¯•ã€å›æ»šä¿æŠ¤ã€åˆ†æ”¯ç®¡ç†
4. **å®æ—¶åé¦ˆ**ï¼šå‡çº§è¿‡ç¨‹ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ¸…æ™°çš„è¿›åº¦åé¦ˆ

è¿™æ · Bor å°±ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œè€Œæ˜¯ä¸€ä¸ªèƒ½å¤Ÿè‡ªæˆ‘å­¦ä¹ å’Œè¿›åŒ–çš„æ™ºèƒ½ç³»ç»Ÿï¼

<function_calls>
<invoke name="userInput">
<parameter name="question">**ç°åœ¨çš„è®¾è®¡æ–‡æ¡£åŒ…å«äº†è‡ªå‡çº§èƒ½åŠ›ï¼Œçœ‹èµ·æ¥å¦‚ä½•ï¼Ÿå¦‚æœæ»¡æ„çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­åˆ›å»ºå®æ–½ä»»åŠ¡åˆ—è¡¨ã€‚**